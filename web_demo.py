# Copyright (c) Alibaba Cloud.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""çµæ¢ - ç¥ç»ç§‘å­¦æ¨ç†é“¾ç”Ÿæˆç³»ç»Ÿ Web Demo"""
import os
# os.environ["GRADIO_TEMP_DIR"] = "/home/cyy/rag/.gradio_tmp"

import asyncio
import tempfile
import json
from datetime import datetime
from argparse import ArgumentParser

import gradio as gr
import mdtex2html

from reasoning_chain_generator import ReasoningChainGenerator
import config


def _get_args():
    parser = ArgumentParser()
    parser.add_argument("--api-key", type=str, default=None,
                        help="Qwen API key (default from config.py)")
    parser.add_argument("--chroma-path", type=str, default="./chroma_db",
                        help="ChromaDB path")
    parser.add_argument("--model", type=str, default="qwen3-max",
                        help="Generation model name")
    parser.add_argument("--share", action="store_true", default=False,
                        help="Create a publicly shareable link for the interface.")
    parser.add_argument("--inbrowser", action="store_true", default=False,
                        help="Automatically launch the interface in a new tab on the default browser.")
    parser.add_argument("--server-port", type=int, default=7201,
                        help="Demo server port.")
    parser.add_argument("--server-name", type=str, default="0.0.0.0",
                        help="Demo server name.")

    args = parser.parse_args()
    return args


def postprocess(self, y):
    if y is None:
        return []
    for i, (message, response) in enumerate(y):
        y[i] = (
            None if message is None else mdtex2html.convert(message),
            None if response is None else mdtex2html.convert(response),
        )
    return y


gr.Chatbot.postprocess = postprocess


def _parse_text(text):
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split("`")
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = f"<br></code></pre>"
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace("`", r"\`")
                    line = line.replace("<", "&lt;")
                    line = line.replace(">", "&gt;")
                    line = line.replace(" ", "&nbsp;")
                    line = line.replace("*", "&ast;")
                    line = line.replace("_", "&lowbar;")
                    line = line.replace("-", "&#45;")
                    line = line.replace(".", "&#46;")
                    line = line.replace("!", "&#33;")
                    line = line.replace("(", "&#40;")
                    line = line.replace(")", "&#41;")
                    line = line.replace("$", "&#36;")
                lines[i] = "<br>" + line
    text = "".join(lines)
    return text


def _summarize_text(text: str, max_length: int = 100) -> str:
    """æ€»ç»“æ–‡æœ¬å†…å®¹ï¼Œä¿ç•™å‰åŠéƒ¨åˆ†"""
    if not text or text == 'N/A':
        return 'N/A'
    
    # å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›
    if len(text) <= max_length:
        return text
    
    # æˆªå–å‰åŠéƒ¨åˆ†å¹¶æ·»åŠ çœç•¥å·
    return text[:max_length] + "..."

def _generate_full_text_file(result: dict, query: str) -> str:
    """ç”ŸæˆåŒ…å«å®Œæ•´æ¨ç†é“¾çš„æ–‡æœ¬æ–‡ä»¶"""
    if not result or result.get('status') != 'success':
        return None
    
    reasoning = result.get('reasoning_chain', {})
    if not reasoning:
        return None
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"reasoning_chain_{timestamp}.txt"
    filepath = os.path.join(tempfile.gettempdir(), filename)
    
    # å†™å…¥å®Œæ•´å†…å®¹
    with open(filepath, 'w', encoding='utf-8') as f:
        
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç ”ç©¶é—®é¢˜: {query}\n")
        f.write("-" * 80 + "\n\n")
        
        # 1. Problem Decomposition (å®Œæ•´å†…å®¹)
        f.write("1. Problem Decomposition (å®Œæ•´å†…å®¹)\n")
        f.write("-" * 80 + "\n")
        f.write(reasoning.get('problem_decomposition', 'N/A') + "\n\n")
        
        # 2. Data Requirements (å®Œæ•´å†…å®¹)
        f.write("2. Data Requirements (å®Œæ•´å†…å®¹)\n")
        f.write("-" * 80 + "\n")
        f.write(reasoning.get('data', 'N/A') + "\n\n")
        
        # 3. Experimental Methods (å®Œæ•´å†…å®¹)
        f.write("3. Experimental Methods (å®Œæ•´å†…å®¹)\n")
        f.write("-" * 80 + "\n")
        f.write(reasoning.get('method', 'N/A') + "\n\n")
        
        # 4. Conclusion (å®Œæ•´å†…å®¹)
        f.write("4. Expected Conclusion (å®Œæ•´å†…å®¹)\n")
        f.write("-" * 80 + "\n")
        f.write(reasoning.get('conclusion', 'N/A') + "\n\n")
        
        # References
        if 'references' in result and result['references']:
            f.write("\n" + "=" * 80 + "\n")
            f.write("å‚è€ƒæ–‡çŒ®\n")
            f.write("=" * 80 + "\n\n")
            
            for i, ref in enumerate(result['references'][:15], 1):
                authors = ref.get('authors', 'Unknown Authors')
                title = ref.get('title', 'Unknown Title')
                journal = ref.get('journal', 'Unknown Journal')
                year = ref.get('year', 'Unknown Year')
                
                f.write(f"[{i}] {authors}. {title}[J]. {journal}, {year}.\n")
                f.write(f"    (ç›¸å…³æ€§: {ref['similarity']:.3f} | å¼•ç”¨æ•°: {ref['citation_count']})\n\n")
    
    return filepath
    
def _format_reasoning_chain(result: dict) -> str:
    """æ ¼å¼åŒ–æ¨ç†é“¾ä¸º Markdownï¼ˆä½¿ç”¨LLMç”Ÿæˆçš„æ‘˜è¦ï¼‰"""
    if result.get('status') == 'error':
        return f"âŒ **é”™è¯¯**: {result.get('message', 'ç”Ÿæˆå¤±è´¥')}"
    
    # ä¼˜å…ˆä½¿ç”¨LLMç”Ÿæˆçš„æ‘˜è¦
    summary = result.get('summary')
    if summary:
        # æ„å»ºæ ¼å¼åŒ–è¾“å‡ºï¼ˆä½¿ç”¨LLMæ‘˜è¦ï¼‰
        output = []
        output.append(f"## ğŸ”¬ ç§‘ç ”æ¨ç†é“¾ï¼ˆæ¦‚è¦ï¼‰\n")
        
        # 1. Problem Decomposition - LLMæ‘˜è¦
        output.append(f"### 1ï¸âƒ£ Problem Decomposition")
        output.append(f"{summary.get('problem_decomposition', 'N/A')}\n")
        
        # 2. Data Requirements - LLMæ‘˜è¦
        output.append(f"### 2ï¸âƒ£ Data Requirements")
        output.append(f"{summary.get('data', 'N/A')}\n")
        
        # 3. Experimental Methods - LLMæ‘˜è¦
        output.append(f"### 3ï¸âƒ£ Experimental Methods")
        output.append(f"{summary.get('method', 'N/A')}\n")
        
        # 4. Conclusion - LLMæ‘˜è¦
        output.append(f"### 4ï¸âƒ£ Expected Conclusion")
        output.append(f"{summary.get('conclusion', 'N/A')}\n")
    
    else:
        # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹æ¨ç†é“¾çš„æˆªæ–­ç‰ˆæœ¬
        reasoning = result.get('reasoning_chain')
        if not reasoning:
            return f"âš ï¸ **æ— æ³•è§£æç”Ÿæˆç»“æœ**\n\nåŸå§‹è¾“å‡º:\n```\n{result.get('raw_output', 'N/A')}\n```"
        
        # æ„å»ºæ ¼å¼åŒ–è¾“å‡ºï¼ˆç®€çŸ­ç‰ˆæœ¬ï¼‰
        output = []
        output.append(f"## ğŸ”¬ ç§‘ç ”æ¨ç†é“¾ï¼ˆæ¦‚è¦ï¼‰\n")
        
        # 1. Problem Decomposition - ç®€çŸ­æ€»ç»“
        problem = reasoning.get('problem_decomposition', 'N/A')
        output.append(f"### 1ï¸âƒ£ Problem Decomposition")
        output.append(f"{_summarize_text(problem, max_length=150)}\n")
        
        # 2. Data Requirements - ç®€çŸ­æ€»ç»“
        data = reasoning.get('data', 'N/A')
        output.append(f"### 2ï¸âƒ£ Data Requirements")
        output.append(f"{_summarize_text(data, max_length=150)}\n")
        
        # 3. Experimental Methods - ç®€çŸ­æ€»ç»“
        method = reasoning.get('method', 'N/A')
        output.append(f"### 3ï¸âƒ£ Experimental Methods")
        output.append(f"{_summarize_text(method, max_length=150)}\n")
        
        # 4. Conclusion - ç®€çŸ­æ€»ç»“
        conclusion = reasoning.get('conclusion', 'N/A')
        output.append(f"### 4ï¸âƒ£ Expected Conclusion")
        output.append(f"{_summarize_text(conclusion, max_length=150)}\n")
    
    return "\n".join(output)


def _launch_demo(args, generator):
    """å¯åŠ¨ Gradio Demo"""

    def predict(_query, _chatbot, _task_history):
        """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œç”Ÿæˆæ¨ç†é“¾"""
        if not _query.strip():
            yield _chatbot, _task_history, None
            return
        
        print(f"User Query: {_query}")
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        _chatbot.append((_query, "ğŸ” æ­£åœ¨è¿›è¡Œé—®é¢˜åˆ†æä¸ç”Ÿæˆ...\n"))
        yield _chatbot, _task_history, None
        
        try:
            # ç”Ÿæˆæ¨ç†é“¾
            result = generator.generate_reasoning_chain(
                research_question=_query,
                top_k=15,
                return_references=True
            )
            
            # å¤„ç†å¯èƒ½çš„å¼‚æ­¥ç»“æœ
            if asyncio.iscoroutine(result):
                print("DEBUG: Detected coroutine result, running asyncio...")
                result = asyncio.run(result)
            elif hasattr(result, "result"):  # é’ˆå¯¹ AsyncRequest
                print("DEBUG: Detected AsyncRequest, resolving...")
                result = result.result()


            # æ ¼å¼åŒ–è¾“å‡ºï¼ˆç®€çŸ­æ€»ç»“ï¼‰
            formatted_response = _format_reasoning_chain(result)
            
            # ç”Ÿæˆå®Œæ•´æ–‡æœ¬æ–‡ä»¶
            full_text_file = _generate_full_text_file(result, _query)
            
            print(f"Generation completed successfully")
            print(f"Response length: {len(formatted_response)} chars")
            print("="*80)
            print("ç”Ÿæˆçš„æ¨ç†é“¾å†…å®¹:")
            print("="*80)
            print(formatted_response)
            print("="*80)
            
            if full_text_file:
                print(f"å®Œæ•´å†…å®¹æ–‡ä»¶: {full_text_file}")
                print(f"æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(full_text_file)}")
                if os.path.exists(full_text_file):
                    print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(full_text_file)} bytes")
            else:
                print("âš ï¸ æ— æ³•ç”Ÿæˆå®Œæ•´æ–‡æœ¬æ–‡ä»¶")
            
            # æ›´æ–°èŠå¤©ç•Œé¢
            _chatbot[-1] = (_query, formatted_response)
            _task_history.append((_query, formatted_response))
            
            print(f"Chatbot updated, current length: {len(_chatbot)}")
            print(f"Task history length: {len(_task_history)}")
            
            # è¿”å›æ›´æ–°åçš„çŠ¶æ€å’Œæ–‡ä»¶ï¼ˆæ˜¾ç¤ºä¸‹è½½ç»„ä»¶ï¼‰
            if full_text_file:
                yield _chatbot, _task_history, gr.update(value=full_text_file, visible=True)
            else:
                yield _chatbot, _task_history, gr.update(visible=False)
            
        except Exception as e:
            error_msg = f"âŒ **ç”Ÿæˆå¤±è´¥**: {str(e)}"
            _chatbot[-1] = (_query, error_msg)
            _task_history.append((_query, error_msg))
            print(f"Error: {e}")
            import traceback
            traceback.print_exc()
            
            yield _chatbot, _task_history, gr.update(visible=False)

    def regenerate(_chatbot, _task_history):
        """é‡æ–°ç”Ÿæˆæœ€åä¸€ä¸ªå›ç­”"""
        if not _task_history:
            yield _chatbot, _task_history, gr.update(visible=False)
            return
        
        # ç§»é™¤æœ€åä¸€è½®å¯¹è¯
        last_query = _task_history[-1][0]
        _task_history.pop(-1)
        _chatbot.pop(-1)
        
        # é‡æ–°ç”Ÿæˆï¼ˆä½¿ç”¨ç›¸åŒçš„predictå‡½æ•°ï¼‰
        for result in predict(last_query, _chatbot, _task_history):
            yield result

    def reset_user_input():
        """æ¸…ç©ºè¾“å…¥æ¡†"""
        return gr.update(value="")

    def reset_state(_chatbot, _task_history):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        _task_history.clear()
        _chatbot.clear()
        return [], [], gr.update(visible=False)

    with gr.Blocks(title="çµæ¢ - ç¥ç»ç§‘å­¦æ¨ç†é“¾ç”Ÿæˆç³»ç»Ÿ") as demo:

        gr.Markdown("""<center><font size=8>ğŸ§  çµæ¢</font></center>""")
        gr.Markdown(
            """\
<center><font size=3>ç§‘å­¦é—®é¢˜ç”Ÿæˆä¸å®éªŒè®¾è®¡å¤§æ¨¡å‹</font></center>
<center><font size=2>åŸºäº CNS æµ·é‡æ–‡çŒ®æ„å»º</font></center>""")

        with gr.Row():
            with gr.Column(scale=4):
                chatbot = gr.Chatbot(label='é—®é¢˜ç”Ÿæˆä¸å®éªŒè®¾è®¡', height=600)
                # æ–‡ä»¶ä¸‹è½½ç»„ä»¶ - åˆå§‹éšè—ï¼Œç”Ÿæˆå†…å®¹åæ˜¾ç¤º
                download_file = gr.File(
                    label="ğŸ“¥ ä¸‹è½½å®Œæ•´æŠ¥å‘Š", 
                    visible=False,
                    height=50,
                )
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ’¡ ä½¿ç”¨è¯´æ˜")
                gr.Markdown("""
                1. è¾“å…¥ï¼šç ”ç©¶é—®é¢˜
                2. è¾“å‡ºï¼š
                   - é—®é¢˜ç”Ÿæˆ
                   - æ•°æ®éœ€æ±‚
                   - å®éªŒæ–¹æ³•
                   - æ½œåœ¨ç»“è®º

                **ç¤ºä¾‹é—®é¢˜**ï¼š
                - How does stress affect hippocampal neurogenesis?
                - What is the role of dopamine in reward learning?
                """)
        
        query = gr.Textbox(
            lines=3, 
            label='è¾“å…¥ä½ çš„ç ”ç©¶é—®é¢˜ (Input Research Question)',
            placeholder="ä¾‹å¦‚: How does chronic stress affect hippocampal neurogenesis?"
        )
        task_history = gr.State([])

        with gr.Row():
            empty_btn = gr.Button("ğŸ§¹ æ¸…é™¤å†å² (Clear)")
            submit_btn = gr.Button("ğŸš€ ç”Ÿæˆæ¨ç†é“¾ (Generate)", variant="primary")
            regen_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ (Regenerate)")
        
        # äº‹ä»¶ç»‘å®š
        submit_btn.click(
            predict, 
            [query, chatbot, task_history], 
            [chatbot, task_history, download_file], 
            show_progress=True,
            queue=True
        )
        submit_btn.click(reset_user_input, [], [query])
        
        # æ”¯æŒå›è½¦æäº¤
        query.submit(
            predict,
            [query, chatbot, task_history],
            [chatbot, task_history, download_file],
            show_progress=True,
            queue=True
        )
        query.submit(reset_user_input, [], [query])
        
        empty_btn.click(
            reset_state, 
            [chatbot, task_history], 
            outputs=[chatbot, task_history, download_file], 
            show_progress=True
        )
        regen_btn.click(
            regenerate, 
            [chatbot, task_history], 
            [chatbot, task_history, download_file], 
            show_progress=True,
            queue=True
        )

        gr.Markdown("""\
<center><font size=2>æœ¬ç³»ç»ŸåŸºäº RAG æŠ€æœ¯ï¼Œç»“åˆå‘é‡æ£€ç´¢ä¸å¤§æ¨¡å‹ç”Ÿæˆ | æ•°æ®æ¥æº: æµ·é‡ CNS æ–‡çŒ®</font></center>
<center><font size=2>ğŸ’¡ æç¤ºï¼šç•Œé¢æ˜¾ç¤ºä¸ºç®€è¦æ€»ç»“ï¼Œå®Œæ•´å†…å®¹è¯·ä¸‹è½½è¯¦ç»†æŠ¥å‘Š</font></center>""")

    demo.queue().launch(
        share=args.share,
        inbrowser=args.inbrowser,
        server_port=args.server_port,
        server_name=args.server_name,
    )


def main():
    """ä¸»å‡½æ•°"""
    args = _get_args()

    # è·å– API key
    api_key = args.api_key or config.OPENAI_API_KEY
    if not api_key:
        raise ValueError("è¯·æä¾› API keyï¼ˆé€šè¿‡ --api-key æˆ–åœ¨ config.py ä¸­é…ç½®ï¼‰")

    print("="*80)
    print("çµæ¢ - ç¥ç»ç§‘å­¦æ¨ç†é“¾ç”Ÿæˆç³»ç»Ÿ")
    print("="*80)
    
    # åˆå§‹åŒ–æ¨ç†é“¾ç”Ÿæˆå™¨
    try:
        generator = ReasoningChainGenerator(
            api_key=api_key,
            chroma_path=args.chroma_path,
            collection_name="neuroscience",
            generation_model=args.model,
            temperature=0.3
        )
        print("âœ“ åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"âœ— åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    print("\næ­£åœ¨å¯åŠ¨ Web ç•Œé¢...")
    print(f"  è®¿é—®åœ°å€: http://{args.server_name}:{args.server_port}")
    if args.share:
        print(f"  å…¬å¼€é“¾æ¥: å°†åœ¨å¯åŠ¨åç”Ÿæˆ")
    print("="*80)
    
    # å¯åŠ¨ Demo
    _launch_demo(args, generator)


if __name__ == '__main__':
    main()
