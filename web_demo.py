# Copyright (c) Alibaba Cloud.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""çµæ¢ - ç¥ç»ç§‘å­¦æ¨ç†é“¾ç”Ÿæˆç³»ç»Ÿ Web Demo"""
import os
os.environ["GRADIO_TEMP_DIR"] = "/home/cyy/rag/.gradio_tmp"

import asyncio
from argparse import ArgumentParser

import gradio as gr
import mdtex2html

from reasoning_chain_generator import ReasoningChainGenerator
import config


def _get_args():
    parser = ArgumentParser()
    parser.add_argument("--api-key", type=str, default=None,
                        help="Qwen API key (default from config.py)")
    parser.add_argument("--chroma-path", type=str, default="./chroma_db",
                        help="ChromaDB path")
    parser.add_argument("--model", type=str, default="qwen3-max",
                        help="Generation model name")
    parser.add_argument("--share", action="store_true", default=False,
                        help="Create a publicly shareable link for the interface.")
    parser.add_argument("--inbrowser", action="store_true", default=False,
                        help="Automatically launch the interface in a new tab on the default browser.")
    parser.add_argument("--server-port", type=int, default=7201,
                        help="Demo server port.")
    parser.add_argument("--server-name", type=str, default="0.0.0.0",
                        help="Demo server name.")

    args = parser.parse_args()
    return args


def postprocess(self, y):
    if y is None:
        return []
    for i, (message, response) in enumerate(y):
        y[i] = (
            None if message is None else mdtex2html.convert(message),
            None if response is None else mdtex2html.convert(response),
        )
    return y


gr.Chatbot.postprocess = postprocess


def _parse_text(text):
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split("`")
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = f"<br></code></pre>"
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace("`", r"\`")
                    line = line.replace("<", "&lt;")
                    line = line.replace(">", "&gt;")
                    line = line.replace(" ", "&nbsp;")
                    line = line.replace("*", "&ast;")
                    line = line.replace("_", "&lowbar;")
                    line = line.replace("-", "&#45;")
                    line = line.replace(".", "&#46;")
                    line = line.replace("!", "&#33;")
                    line = line.replace("(", "&#40;")
                    line = line.replace(")", "&#41;")
                    line = line.replace("$", "&#36;")
                lines[i] = "<br>" + line
    text = "".join(lines)
    return text


def _format_reasoning_chain(result: dict) -> str:
    """æ ¼å¼åŒ–æ¨ç†é“¾ä¸º Markdown"""
    if result.get('status') == 'error':
        return f"âŒ **é”™è¯¯**: {result.get('message', 'ç”Ÿæˆå¤±è´¥')}"
    
    reasoning = result.get('reasoning_chain')
    if not reasoning:
        return f"âš ï¸ **æ— æ³•è§£æç”Ÿæˆç»“æœ**\n\nåŸå§‹è¾“å‡º:\n```\n{result.get('raw_output', 'N/A')}\n```"
    
    # æ„å»ºæ ¼å¼åŒ–è¾“å‡º
    output = []
    output.append(f"## ğŸ”¬ ç§‘ç ”æ¨ç†é“¾\n")
    
    # 1. Problem Decomposition
    output.append(f"### 1ï¸âƒ£ Problem Decomposition")
    output.append(f"{reasoning.get('problem_decomposition', 'N/A')}\n")
    
    # 2. Data Requirements
    output.append(f"### 2ï¸âƒ£ Data Requirements")
    output.append(f"{reasoning.get('data', 'N/A')}\n")
    
    # 3. Experimental Methods
    output.append(f"### 3ï¸âƒ£ Experimental Methods")
    output.append(f"{reasoning.get('method', 'N/A')}\n")
    
    # 4. Conclusion
    output.append(f"### 4ï¸âƒ£ Expected Conclusion")
    output.append(f"{reasoning.get('conclusion', 'N/A')}\n")
    
    # References
    if 'references' in result and result['references']:
        output.append(f"---\n### ğŸ“š å‚è€ƒæ–‡çŒ®")
        for i, ref in enumerate(result['references'][:3], 1):
            output.append(f"{i}. **{ref['title']}** ({ref['year']})")
            output.append(f"   - ç›¸ä¼¼åº¦: {ref['similarity']:.3f} | å¼•ç”¨æ•°: {ref['citation_count']}")
    
    return "\n".join(output)


def _launch_demo(args, generator):
    """å¯åŠ¨ Gradio Demo"""

    def predict(_query, _chatbot, _task_history):
        """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œç”Ÿæˆæ¨ç†é“¾"""
        if not _query.strip():
            return _chatbot, _task_history
        
        print(f"User Query: {_query}")
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶æ˜¾ç¤ºåŠ è½½çŠ¶æ€
        _chatbot.append((_query, "ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³ç ”ç©¶...\n"))
        yield _chatbot, _task_history
        
        try:
            # ç”Ÿæˆæ¨ç†é“¾
            result = generator.generate_reasoning_chain(
                research_question=_query,
                top_k=5,
                return_references=True
            )
            
            # å¤„ç†å¯èƒ½çš„å¼‚æ­¥ç»“æœ
            if asyncio.iscoroutine(result):
                print("DEBUG: Detected coroutine result, running asyncio...")
                result = asyncio.run(result)
            elif hasattr(result, "result"):  # é’ˆå¯¹ AsyncRequest
                print("DEBUG: Detected AsyncRequest, resolving...")
                result = result.result()


            # æ ¼å¼åŒ–è¾“å‡º
            formatted_response = _format_reasoning_chain(result)
            
            print(f"Generation completed successfully")
            print(f"Response length: {len(formatted_response)} chars")
            print("="*80)
            print("ç”Ÿæˆçš„æ¨ç†é“¾å†…å®¹:")
            print("="*80)
            print(formatted_response)
            print("="*80)
            
            # æ›´æ–°èŠå¤©ç•Œé¢
            _chatbot[-1] = (_query, formatted_response)
            _task_history.append((_query, formatted_response))
            
            print(f"Chatbot updated, current length: {len(_chatbot)}")
            print(f"Task history length: {len(_task_history)}")
            
        except Exception as e:
            error_msg = f"âŒ **ç”Ÿæˆå¤±è´¥**: {str(e)}"
            _chatbot[-1] = (_query, error_msg)
            _task_history.append((_query, error_msg))
            print(f"Error: {e}")
            import traceback
            traceback.print_exc()

        print("DEBUG: predict() reached return point")
        # è¿”å›æ›´æ–°åçš„çŠ¶æ€
        yield _chatbot, _task_history

    def regenerate(_chatbot, _task_history):
        """é‡æ–°ç”Ÿæˆæœ€åä¸€ä¸ªå›ç­”"""
        if not _task_history:
            yield _chatbot, _task_history
            return
        
        # ç§»é™¤æœ€åä¸€è½®å¯¹è¯
        last_query = _task_history[-1][0]
        _task_history.pop(-1)
        _chatbot.pop(-1)
        
        # é‡æ–°ç”Ÿæˆ
        yield from predict(last_query, _chatbot, _task_history)

    def reset_user_input():
        """æ¸…ç©ºè¾“å…¥æ¡†"""
        return gr.update(value="")

    def reset_state(_chatbot, _task_history):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        _task_history.clear()
        _chatbot.clear()
        return [], []

    with gr.Blocks(title="çµæ¢ - ç¥ç»ç§‘å­¦æ¨ç†é“¾ç”Ÿæˆç³»ç»Ÿ") as demo:

        gr.Markdown("""<center><font size=8>ğŸ§  çµæ¢</font></center>""")
        gr.Markdown(
            """\
<center><font size=3>ç¥ç»ç§‘å­¦å®éªŒè®¾è®¡æ¨ç†é“¾ç”Ÿæˆç³»ç»Ÿ</font></center>
<center><font size=2>åŸºäº Nature Neuroscience 2729 ç¯‡è®ºæ–‡æ„å»º | RAG + LLM</font></center>""")

        with gr.Row():
            with gr.Column(scale=4):
                chatbot = gr.Chatbot(label='æ¨ç†é“¾ç”Ÿæˆ', height=600)
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ’¡ ä½¿ç”¨è¯´æ˜")
                gr.Markdown("""
                1. **è¾“å…¥ç ”ç©¶é—®é¢˜**ï¼ˆè‹±æ–‡ï¼‰
                2. ç³»ç»Ÿè‡ªåŠ¨æ£€ç´¢ç›¸å…³ç ”ç©¶
                3. ç”Ÿæˆå®Œæ•´æ¨ç†é“¾ï¼š
                   - é—®é¢˜åˆ†è§£
                   - æ•°æ®éœ€æ±‚
                   - å®éªŒæ–¹æ³•
                   - é¢„æœŸç»“è®º
                
                **ç¤ºä¾‹é—®é¢˜**ï¼š
                - How does stress affect hippocampal neurogenesis?
                - What is the role of dopamine in reward learning?
                """)
        
        query = gr.Textbox(
            lines=3, 
            label='è¾“å…¥ä½ çš„ç ”ç©¶é—®é¢˜ (Input Research Question)',
            placeholder="ä¾‹å¦‚: How does chronic stress affect hippocampal neurogenesis?"
        )
        task_history = gr.State([])

        with gr.Row():
            empty_btn = gr.Button("ğŸ§¹ æ¸…é™¤å†å² (Clear)")
            submit_btn = gr.Button("ğŸš€ ç”Ÿæˆæ¨ç†é“¾ (Generate)", variant="primary")
            regen_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ (Regenerate)")

        # äº‹ä»¶ç»‘å®š
        submit_btn.click(
            predict, 
            [query, chatbot, task_history], 
            [chatbot, task_history], 
            show_progress=True,
            queue=True
        )
        submit_btn.click(reset_user_input, [], [query])
        
        # æ”¯æŒå›è½¦æäº¤
        query.submit(
            predict,
            [query, chatbot, task_history],
            [chatbot, task_history],
            show_progress=True,
            queue=True
        )
        query.submit(reset_user_input, [], [query])
        
        empty_btn.click(
            reset_state, 
            [chatbot, task_history], 
            outputs=[chatbot, task_history], 
            show_progress=True
        )
        regen_btn.click(
            regenerate, 
            [chatbot, task_history], 
            [chatbot, task_history], 
            show_progress=True
        )

        gr.Markdown("""\
<center><font size=2>æœ¬ç³»ç»ŸåŸºäº RAG æŠ€æœ¯ï¼Œç»“åˆå‘é‡æ£€ç´¢ä¸å¤§æ¨¡å‹ç”Ÿæˆ | æ•°æ®æ¥æº: Nature Neuroscience</font></center>""")

    demo.queue().launch(
        share=args.share,
        inbrowser=args.inbrowser,
        server_port=args.server_port,
        server_name=args.server_name,
    )


def main():
    """ä¸»å‡½æ•°"""
    args = _get_args()

    # è·å– API key
    api_key = args.api_key or config.OPENAI_API_KEY
    if not api_key:
        raise ValueError("è¯·æä¾› API keyï¼ˆé€šè¿‡ --api-key æˆ–åœ¨ config.py ä¸­é…ç½®ï¼‰")

    print("="*80)
    print("çµæ¢ - ç¥ç»ç§‘å­¦æ¨ç†é“¾ç”Ÿæˆç³»ç»Ÿ")
    print("="*80)
    print(f"æ­£åœ¨åˆå§‹åŒ–...")
    print(f"  æ•°æ®åº“è·¯å¾„: {args.chroma_path}")
    print(f"  ç”Ÿæˆæ¨¡å‹: {args.model}")
    
    # åˆå§‹åŒ–æ¨ç†é“¾ç”Ÿæˆå™¨
    try:
        generator = ReasoningChainGenerator(
            api_key=api_key,
            chroma_path=args.chroma_path,
            collection_name="neuroscience",
            generation_model=args.model,
            temperature=0.7
        )
        print("âœ“ åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"âœ— åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    print("\næ­£åœ¨å¯åŠ¨ Web ç•Œé¢...")
    print(f"  è®¿é—®åœ°å€: http://{args.server_name}:{args.server_port}")
    if args.share:
        print(f"  å…¬å¼€é“¾æ¥: å°†åœ¨å¯åŠ¨åç”Ÿæˆ")
    print("="*80)
    
    # å¯åŠ¨ Demo
    _launch_demo(args, generator)


if __name__ == '__main__':
    main()
