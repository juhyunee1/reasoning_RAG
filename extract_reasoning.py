"""
ç›´æ¥ä»è®ºæ–‡æå–ç§‘ç ”æ¨ç†é“¾
ç”Ÿæˆreasoning_chains.jsonlæ ¼å¼
"""

import json
import uuid
from pathlib import Path
from typing import List, Dict, Optional
from openai import OpenAI
from datetime import datetime
from tqdm import tqdm

class DirectReasoningExtractor:
    """ç›´æ¥æå–ç§‘ç ”æ¨ç†é“¾"""
    
    def __init__(self, api_key: str, model: str = "qwen3-max"):
        """åˆå§‹åŒ–æå–å™¨"""
        self.api_key = api_key
        self.model = model
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        # å…³é”®ç« èŠ‚ï¼ˆç”¨äºæå–æ¨ç†é“¾ï¼‰
        # æ³¨æ„ï¼šNature Neuroscienceçš„Introductionç« èŠ‚å«åš"Main"
        self.key_sections = ['abstract', 'introduction', 'main', 'results', 'discussion', 'conclusion', 'methods']
    
    def load_metadata(self, metadata_file: Path) -> Dict[str, Dict]:
        """åŠ è½½æ‰€æœ‰è®ºæ–‡çš„å…ƒæ•°æ®"""
        print("åŠ è½½å…ƒæ•°æ®...")
        metadata_dict = {}
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–metadata"):
                if line.strip():
                    meta = json.loads(line)
                    metadata_dict[meta['id']] = meta
        
        print(f"âœ“ åŠ è½½äº† {len(metadata_dict)} ç¯‡è®ºæ–‡çš„å…ƒæ•°æ®")
        return metadata_dict
    
    def load_media(self, media_file: Path) -> Dict[str, List]:
        """
        åŠ è½½æ‰€æœ‰è®ºæ–‡çš„åª’ä½“æ•°æ®ï¼ˆfigures, tablesç­‰ï¼‰
        
        æ³¨æ„ï¼šmedia.jsonlä¸­æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå›¾è¡¨ï¼ŒåŒä¸€ç¯‡è®ºæ–‡æœ‰å¤šè¡Œ
        éœ€è¦æŒ‰paper_idèšåˆ
        """
        print("åŠ è½½åª’ä½“æ•°æ®...")
        media_dict = {}
        
        with open(media_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–media"):
                if line.strip():
                    media_item = json.loads(line)
                    paper_id = media_item.get('id')
                    
                    if paper_id:
                        # å°†æ¯ä¸ªmedia itemæ·»åŠ åˆ°å¯¹åº”paperçš„åˆ—è¡¨ä¸­
                        if paper_id not in media_dict:
                            media_dict[paper_id] = []
                        
                        # æå–å…³é”®ä¿¡æ¯
                        media_dict[paper_id].append({
                            'type': media_item.get('type'),
                            'label': media_item.get('label'),
                            'caption': media_item.get('caption'),
                            'legend': media_item.get('legend'),
                            'name': media_item.get('name')
                        })
        
        print(f"âœ“ åŠ è½½äº† {len(media_dict)} ç¯‡è®ºæ–‡çš„åª’ä½“æ•°æ®")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_media = sum(len(items) for items in media_dict.values())
        print(f"âœ“ æ€»è®¡ {total_media} ä¸ªå›¾è¡¨")
        
        return media_dict
    
    def parse_contents_file(self, contents_file: Path, max_papers: int = None) -> List[Dict]:
        """
        è§£æcontents.jsonlæ–‡ä»¶ï¼ˆä½¿ç”¨çŠ¶æ€æœºå¤„ç†å­—ç¬¦ä¸²ä¸­çš„æ‹¬å·ï¼‰
        """
        print(f"\nè§£æcontentsæ–‡ä»¶...")
        papers = []
        
        with open(contents_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        pos = 0
        paper_count = 0
        failed_count = 0
        
        with tqdm(desc="è§£æè®ºæ–‡") as pbar:
            while pos < len(content):
                # è·³è¿‡ç©ºç™½å­—ç¬¦
                while pos < len(content) and content[pos] in ' \t\n\r':
                    pos += 1
                
                if pos >= len(content):
                    break
                
                # æ‰¾åˆ°JSONå¯¹è±¡çš„èµ·å§‹ä½ç½®
                if content[pos] != '{':
                    pos += 1
                    continue
                
                start = pos
                brace_count = 0
                in_string = False
                escape = False
                
                # çŠ¶æ€æœºï¼šæ­£ç¡®å¤„ç†å­—ç¬¦ä¸²ä¸­çš„æ‹¬å·
                for i in range(start, len(content)):
                    char = content[i]
                    
                    if escape:
                        escape = False
                        continue
                    
                    if char == '\\':
                        escape = True
                        continue
                    
                    if char == '"':
                        in_string = not in_string
                        continue
                    
                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end = i + 1
                                
                                try:
                                    paper = json.loads(content[start:end])
                                    papers.append(paper)
                                    paper_count += 1
                                    pbar.update(1)
                                    
                                    if max_papers and paper_count >= max_papers:
                                        print(f"âœ“ å·²è¾¾åˆ°è®¾å®šçš„æ•°é‡é™åˆ¶: {max_papers} ç¯‡")
                                        print(f"âœ“ æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
                                        return papers
                                        
                                except json.JSONDecodeError as e:
                                    failed_count += 1
                                    if failed_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                                        tqdm.write(f"âš  è·³è¿‡è§£æå¤±è´¥çš„å¯¹è±¡ #{paper_count+failed_count}: {str(e)[:80]}")
                                
                                pos = end
                                break
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·ï¼Œè¯´æ˜æ–‡ä»¶ç»“æŸæˆ–æ ¼å¼é”™è¯¯
                    break
        
        if failed_count > 0:
            print(f"âš  å…±æœ‰ {failed_count} ä¸ªå¯¹è±¡è§£æå¤±è´¥ï¼ˆå·²è·³è¿‡ï¼‰")
        print(f"âœ“ æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def extract_key_content(self, paper_content: Dict, media_data: List = None, max_total_chars: int = 20000) -> str:
        """
        æ™ºèƒ½æå–è®ºæ–‡çš„å…³é”®å†…å®¹ç”¨äºæ¨ç†é“¾åˆ†æ
        
        ç­–ç•¥ï¼š
        1. æŒ‰ç« èŠ‚ä¼˜å…ˆçº§åˆ†é…å­—ç¬¦é…é¢
        2. Figuresæ’å…¥åˆ°Methodså’ŒResultsä¹‹é—´
        3. ä¿è¯Resultså’ŒDiscussionä¸è¢«æˆªæ–­
        
        Args:
            paper_content: è®ºæ–‡å†…å®¹
            media_data: åª’ä½“æ•°æ®ï¼ˆfigures, tablesç­‰ï¼‰
            max_total_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶
            
        Returns:
            å…³é”®å†…å®¹çš„æ–‡æœ¬
        """
        # å­—ç¬¦é…é¢åˆ†é…ï¼ˆç¡®ä¿å…³é”®ç« èŠ‚ä¸è¢«æˆªæ–­ï¼‰
        char_quotas = {
            'introduction': 3000,  # Introductionå®Œæ•´ä¿ç•™
            'main': 3000,          # Nature Neuroscienceçš„Introduction
            'methods': 10000,       # Methodsæ ¸å¿ƒéƒ¨åˆ†
            'results': 10000,       # Resultså®Œæ•´ä¿ç•™
            'discussion': 6000     # Discussionå®Œæ•´ä¿ç•™
        }
        
        # åˆ†åˆ«æå–å„ç« èŠ‚å†…å®¹
        sections_dict = {}
        
        for section in paper_content.get('sections', []):
            section_title = section.get('section_title', '')
            section_title_lower = section_title.lower()
            section_texts = section.get('section_text', [])
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®ç« èŠ‚
            matched_key = None
            for key in char_quotas.keys():
                if key in section_title_lower:
                    matched_key = key
                    break
            
            if matched_key:
                section_content = []
                char_count = 0
                quota = char_quotas[matched_key]
                
                # æå–å†…å®¹ç›´åˆ°è¾¾åˆ°é…é¢
                for text_item in section_texts:
                    if isinstance(text_item, dict):
                        text = text_item.get('content', '')
                    else:
                        text = str(text_item)
                    
                    text = text.strip()
                    if text:
                        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é…é¢
                        if char_count + len(text) > quota:
                            # å¦‚æœæ˜¯Resultsæˆ–Discussionï¼Œå¼ºåˆ¶åŒ…å«
                            if matched_key in ['results', 'discussion']:
                                section_content.append(text)
                                char_count += len(text)
                            else:
                                break
                        else:
                            section_content.append(text)
                            char_count += len(text)
                
                if section_content:
                    # æ ¹æ®ç« èŠ‚ç±»å‹æ·»åŠ æ ‡è®°
                    if matched_key in ['introduction', 'main']:
                        label = "[IMPORTANT FOR PROBLEM ANALYSIS]"
                    elif matched_key == 'methods':
                        label = "[IMPORTANT FOR DATA & METHODS]"
                    elif matched_key == 'results':
                        label = "[IMPORTANT FOR CONCLUSION - FINDINGS]"
                    elif matched_key == 'discussion':
                        label = "[IMPORTANT FOR CONCLUSION - SIGNIFICANCE]"
                    else:
                        label = ""
                    
                    sections_dict[matched_key] = f"\n## {section_title} {label}:\n" + "\n".join(section_content)
        
        # æå–Figureså’ŒTablesçš„captions
        figures_content = ""
        if media_data:
            figure_captions = []
            for media_item in media_data[:15]:  # æœ€å¤šæå–15ä¸ªå›¾è¡¨
                media_type = media_item.get('type', '').lower()
                caption = media_item.get('caption', '')
                legend = media_item.get('legend', '')
                name = media_item.get('name', '')
                
                if caption or legend or name:
                    caption_text = f"{media_type.upper()}: "
                    if name:
                        caption_text += f"{name}. "
                    if caption:
                        caption_text += caption
                    if legend:
                        caption_text += f" [Legend: {legend}]"
                    figure_captions.append(caption_text.strip())
            
            if figure_captions:
                figures_content = f"\n## Figures and Tables [IMPORTANT FOR DATA & METHODS]:\n" + "\n".join(figure_captions)
        
        # æŒ‰ä¼˜å…ˆé¡ºåºç»„è£…å†…å®¹ï¼šIntroduction â†’ Methods â†’ Figures â†’ Results â†’ Discussion
        final_parts = []
        
        # 1. Introduction/Main
        if 'introduction' in sections_dict:
            final_parts.append(sections_dict['introduction'])
        elif 'main' in sections_dict:
            final_parts.append(sections_dict['main'])
        
        # 2. Methods
        if 'methods' in sections_dict:
            final_parts.append(sections_dict['methods'])
        
        # 3. Figuresï¼ˆæ’å…¥åˆ°Methodså’ŒResultsä¹‹é—´ï¼‰
        if figures_content:
            final_parts.append(figures_content)
        
        # 4. Resultsï¼ˆæœ€é‡è¦ï¼Œå®Œæ•´ä¿ç•™ï¼‰
        if 'results' in sections_dict:
            final_parts.append(sections_dict['results'])
        
        # 5. Discussionï¼ˆæœ€é‡è¦ï¼Œå®Œæ•´ä¿ç•™ï¼‰
        if 'discussion' in sections_dict:
            final_parts.append(sections_dict['discussion'])
        
        return "\n".join(final_parts)
    
    def build_reasoning_prompt(self, paper_title: str, key_content: str) -> str:
        prompt = f"""You are an expert in scientific reasoning and research methodology in neuroscience. Your task is to extract the CORE SCIENTIFIC REASONING CHAIN from this paper in a CONCISE, STREAMLINED format.

Paper Title
{paper_title}

Key Content from Paper
{key_content[:30000]}  

Your Task
Extract the CORE reasoning logic in 4 CONCISE PARAGRAPHS. Focus on the ESSENCE, not details.

1. problem_decomposition: Core Research Question and Its Logical Breakdown
Read the Introduction/Main section strategically (early â†’ middle â†’ late paragraphs) and write ONE CONCISE PARAGRAPH (3-5 sentences) that captures:
- Early paragraphs: The broad background context and macroscopic problem
- Middle paragraphs: The mechanistic/phenotypic gap or unresolved question
- Late paragraphs: The specific hypothesis or core research question this paper addresses
Synthesize these three layers into a coherent logical flow: background â†’ gap â†’ hypothesis.

2. data: Data Sources and Requirements
Read the Methods section and Figure/Table captions and write ONE CONCISE PARAGRAPH (3-4 sentences) describing what data this research depends on. Include:
- Sample source: What subjects/models were used (species, age, strain, sample size)
- Data type: What was measured (neural recordings, behavior, imaging, molecular data)
- Sampling characteristics: Recording methods, temporal/spatial resolution, duration and brain regions
- Task conditions: What behavioral paradigms or experimental manipulations were applied
Use natural, flowing language to describe the data foundation of this study.

3. method: Experimental Design and Data Acquisition Methods
Read the Methods, Figure captions, and Results sections and write ONE CONCISE PARAGRAPH (4-5 sentences) describing HOW the required data were obtained. Include:
- Experimental design: Control vs. experimental groups, within/between-subject design, sample size rationale
- Data acquisition methods: Specific techniques and instruments used to collect each data type (e.g., recording setup, behavioral tracking, imaging parameters)
- Experimental conditions: Manipulated variables, control conditions, timing/sequence of interventions
- Analytical pipeline: Key processing and analysis steps linking raw data to testable predictions
Focus on the DESIGN LOGIC: what methods were chosen to obtain what data, and how the experimental design allows testing the hypothesis.

4. conclusion: Key Findings, Answer, and Significance
Read the Results and Discussion sections and write ONE CONCISE PARAGRAPH (4-6 sentences) that:
- Summarizes 2-3 main empirical findings
- States how these findings answer the core question
- Explains the broader scientific significance

Critical Instructions:
- In Nature Neuroscience papers, the "Main" section IS the Introduction
- Each field should be ONE CONCISE PARAGRAPH (not lists, not multiple paragraphs)
- Focus on CORE REASONING LOGIC, not exhaustive details
- Write in natural, flowing language suitable for embedding models
- Output in English

Output Format (JSON):
{{
    "problem_decomposition": "...",
    "data": "...",
    "method": "...",
    "conclusion": "..."
}}

Example Output:
{{
    "problem_decomposition": "Synapse development requires coordinated assembly of pre- and postsynaptic components at precise subcellular locations. While neuroligin-neurexin complexes are established synaptogenic pairs, the diversity of synaptic connections suggests additional trans-synaptic adhesion systems remain to be discovered. The mechanistic gap lies in identifying novel heterophilic receptor-ligand pairs that not only mediate cell-cell adhesion but also recruit intracellular scaffolds like PSD-95 to organize functional synapses. This leads to the hypothesis that NGL (netrin-G ligand) proteins, as postsynaptic adhesion molecules binding both presynaptic netrin-G and postsynaptic PSD-95, constitute a bidirectional synaptogenic system regulating excitatory synapse formation.",
    "data": "The study used cultured rat hippocampal neurons (embryonic day 18) and transfected HEK293T cells as experimental models, with n=30-50 neurons per condition across three independent cultures. Protein localization was assessed via confocal immunofluorescence and postembedding immunoelectron microscopy targeting synaptic markers (PSD-95, synapsin I, VGlut1). Functional measurements included whole-cell patch-clamp recordings of miniature excitatory postsynaptic currents (mEPSCs) to quantify synapse number and strength. Key manipulations included lentiviral overexpression, siRNA-mediated knockdown of endogenous NGL-2, and synaptogenic bead assays where NGL-coated microspheres were applied to axons to test presynaptic differentiation capacity.",
    "method": "The research employed a multi-level experimental strategy integrating molecular, structural, and functional analyses. NGL-2 was first identified as a PSD-95 interactor via yeast two-hybrid screening and validated by coimmunoprecipitation from brain lysates. To test synaptogenic function, neurons were transfected with NGL-2 or control vectors at DIV7 and analyzed at DIV14 for changes in synaptic puncta density and mEPSC frequency. Loss-of-function experiments used lentiviral shRNA to knock down endogenous NGL-2 from DIV5-14, followed by blind quantification of dendritic spine density and electrophysiology. Cell-surface presentation assays tested sufficiency: NGL-2-expressing HEK293 cells or antibody-coated beads were cocultured with neurons to induce presynaptic differentiation visualized by synapsin clustering. Statistical comparisons used one-way ANOVA with post-hoc Tukey tests.",
    "conclusion": "NGL-2 localizes specifically to excitatory postsynaptic sites and mediates trans-synaptic adhesion by binding presynaptic netrin-G2 in an isoform-specific manner while simultaneously recruiting PSD-95 via its C-terminal PDZ-binding motif. Overexpression increased excitatory synapse density and mEPSC frequency by 40%, whereas siRNA knockdown reduced synapse number by 35% without affecting inhibitory synapses, demonstrating selective regulation of excitatory synaptogenesis. Application of soluble NGL-2 ectodomain competitively disrupted existing synapses, confirming its necessity for synapse maintenance. These findings establish the NGL-netrin-G complex as a novel trans-synaptic organizing system parallel to neurexin-neuroligin, revealing molecular diversity in synapse specification with implications for understanding circuit wiring, synaptic plasticity, and neurodevelopmental disorders like autism spectrum disorders."
}}

Return ONLY the JSON object with 4 fields, no additional text.
"""
        return prompt
    
    def extract_reasoning_chain(self, paper_content: Dict, metadata: Dict, media_data: List = None) -> Optional[Dict]:
        """
        ä»å•ç¯‡è®ºæ–‡ç›´æ¥æå–æ¨ç†é“¾
        
        Args:
            paper_content: è®ºæ–‡å†…å®¹
            metadata: è®ºæ–‡å…ƒæ•°æ®
            media_data: åª’ä½“æ•°æ®ï¼ˆfigures, tablesç­‰ï¼‰
            
        Returns:
            æ¨ç†é“¾æ•°æ®ï¼ˆåŒ…å«research_reasoningï¼‰
        """
        paper_title = paper_content.get('title', 'Unknown')
        
        # æå–å…³é”®å†…å®¹ï¼ˆåŒ…æ‹¬media captionsï¼‰
        key_content = self.extract_key_content(paper_content, media_data)
        
        if not key_content or len(key_content) < 200:
            print(f"  âœ— å†…å®¹å¤ªå°‘ï¼Œè·³è¿‡")
            return None
        
        # æ„å»ºPrompt
        prompt = self.build_reasoning_prompt(paper_title, key_content)
        
        # è°ƒç”¨LLMæå–æ¨ç†é“¾
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are an expert in scientific reasoning extraction. Always respond with valid JSON."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
                
                reasoning = json.loads(response.choices[0].message.content)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                required_fields = ['problem_decomposition', 'data', 'method', 'conclusion']
                if not all(field in reasoning for field in required_fields):
                    raise ValueError(f"Missing required fields in LLM response")
                
                # æ„å»ºå®Œæ•´æ•°æ®ï¼ˆLLM ç›´æ¥è¿”å›æ®µè½æ ¼å¼ï¼‰
                result = {
                    "paper_id": paper_content.get('id'),
                    "doi": paper_content.get('doi'),
                    "title": paper_title,
                    "journal": metadata.get('journal'),
                    "year": metadata.get('publish_year', '').split('/')[0] if metadata.get('publish_year') else '',
                    "citation_count": metadata.get('citation_count'),
                    "is_open_access": metadata.get('is_open_access'),
                    "authors": metadata.get('authors', []),
                    "article_url": metadata.get('article_url'),
                    
                    # æ ¸å¿ƒæ¨ç†é“¾ï¼ˆç²¾ç‚¼çš„æ®µè½æ ¼å¼ï¼‰
                    "problem_decomposition": reasoning['problem_decomposition'],
                    "data": reasoning['data'],
                    "method": reasoning['method'],
                    "conclusion": reasoning['conclusion']
                }
                
                return result
                
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"  âœ— æå–å¤±è´¥: {e}")
                    return None
        
        return None
    
    def _load_processed_papers(self, output_file: Path) -> set:
        """
        ä»è¾“å‡ºæ–‡ä»¶ä¸­åŠ è½½å·²å¤„ç†çš„è®ºæ–‡ID
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å·²å¤„ç†çš„paper_idé›†åˆ
        """
        processed_ids = set()
        if output_file.exists():
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            if 'paper_id' in data:
                                processed_ids.add(data['paper_id'])
                        except:
                            continue
            except Exception as e:
                print(f"âš  è­¦å‘Š: è¯»å–å·²æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
        
        return processed_ids
    
    def _save_single_result(self, result: Dict, output_file: Path):
        """
        å¢é‡ä¿å­˜å•ä¸ªç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        
        Args:
            result: å•ä¸ªè®ºæ–‡çš„æ¨ç†é“¾
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    def _save_failed_log(self, failed_list: list, output_file: Path):
        """
        ä¿å­˜å¤±è´¥æ—¥å¿—
        
        Args:
            failed_list: å¤±è´¥çš„è®ºæ–‡åˆ—è¡¨ [{paper_id, title, error}, ...]
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        log_file = output_file.parent / "failed_papers.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({
                "total_failed": len(failed_list),
                "failed_papers": failed_list,
                "timestamp": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
    
    def batch_process(
        self,
        data_dir: Path,
        output_file: Path,
        max_papers: int = None,
        resume: bool = True
    ):
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            output_file: è¾“å‡ºæ–‡ä»¶ï¼ˆreasoning_chains.jsonlï¼‰
            max_papers: æœ€å¤šå¤„ç†å¤šå°‘ç¯‡
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤Trueï¼‰
        """
        data_dir = Path(data_dir)
        output_file = Path(output_file)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        print("=" * 80)
        print("ğŸ§  æå–ç§‘ç ”æ¨ç†é“¾ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
        print("=" * 80)
        
        # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
        processed_ids = set()
        if resume:
            processed_ids = self._load_processed_papers(output_file)
            if processed_ids:
                print(f"âœ“ å‘ç°å·²å¤„ç†çš„è®ºæ–‡: {len(processed_ids)} ç¯‡")
                print(f"âœ“ å°†è·³è¿‡å·²å¤„ç†çš„è®ºæ–‡ï¼Œä»æ–­ç‚¹ç»§ç»­")
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_dict = self.load_metadata(data_dir / "metadata.jsonl")
        
        # åŠ è½½åª’ä½“æ•°æ®
        media_dict = {}
        media_file = data_dir / "media.jsonl"
        if media_file.exists():
            media_dict = self.load_media(media_file)
        else:
            print("âš  è­¦å‘Š: æœªæ‰¾åˆ°media.jsonlï¼Œå°†ä¸æå–å›¾è¡¨ä¿¡æ¯")
        
        # è§£æcontents
        papers = self.parse_contents_file(
            data_dir / "contents.jsonl",
            max_papers=max_papers
        )
        
        if not papers:
            print("âœ— é”™è¯¯: æœªæ‰¾åˆ°è®ºæ–‡æ•°æ®")
            return
        
        # è¿‡æ»¤å·²å¤„ç†çš„è®ºæ–‡
        if resume and processed_ids:
            papers_to_process = [p for p in papers if p.get('id') not in processed_ids]
            print(f"\nâœ“ æ€»è®ºæ–‡æ•°: {len(papers)} ç¯‡")
            print(f"âœ“ å·²å¤„ç†: {len(processed_ids)} ç¯‡")
            print(f"âœ“ å¾…å¤„ç†: {len(papers_to_process)} ç¯‡")
        else:
            papers_to_process = papers
            print(f"\nâœ“ å°†å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡")
        
        if not papers_to_process:
            print("\nâœ“ æ‰€æœ‰è®ºæ–‡å·²å¤„ç†å®Œæˆï¼")
            return
        
        print("=" * 80)
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡ï¼ˆå¢é‡ä¿å­˜ï¼‰
        processed = 0
        failed = []
        
        for paper in tqdm(papers_to_process, desc="æå–è¿›åº¦"):
            try:
                paper_id = paper.get('id')
                paper_title = paper.get('title', 'Unknown')
                metadata = metadata_dict.get(paper_id, {})
                media_data = media_dict.get(paper_id, [])
                
                print(f"\nå¤„ç†: {paper_title[:60]}...")
                if media_data:
                    print(f"  æ‰¾åˆ° {len(media_data)} ä¸ªå›¾è¡¨")
                
                # æå–æ¨ç†é“¾
                result = self.extract_reasoning_chain(paper, metadata, media_data)
                
                if result:
                    # ç«‹å³ä¿å­˜ï¼ˆå¢é‡å†™å…¥ï¼‰
                    self._save_single_result(result, output_file)
                    print(f"  âœ“ æå–æˆåŠŸå¹¶å·²ä¿å­˜")
                    processed += 1
                else:
                    failed.append({
                        "paper_id": paper_id,
                        "title": paper_title,
                        "error": "æå–å¤±è´¥"
                    })
                
            except Exception as e:
                error_msg = str(e)
                print(f"\nâœ— å¤„ç†å¤±è´¥: {paper.get('title', 'Unknown')[:40]}")
                print(f"   é”™è¯¯: {error_msg}")
                failed.append({
                    "paper_id": paper.get('id'),
                    "title": paper.get('title', 'Unknown'),
                    "error": error_msg
                })
                continue
        
        # ä¿å­˜å¤±è´¥æ—¥å¿—
        if failed:
            self._save_failed_log(failed, output_file)
            print(f"\nâš  å¤±è´¥æ—¥å¿—å·²ä¿å­˜: {output_file.parent / 'failed_papers.json'}")
        
        # æ€»ç»“
        total_processed = len(processed_ids) + processed
        print("\n" + "=" * 80)
        print("ğŸ“Š å¤„ç†æ€»ç»“")
        print("=" * 80)
        print(f"âœ“ æœ¬æ¬¡å¤„ç†: {processed}/{len(papers_to_process)} ç¯‡")
        print(f"âœ“ ç´¯è®¡å·²å¤„ç†: {total_processed}/{len(papers)} ç¯‡è®ºæ–‡")
        print(f"âœ— æœ¬æ¬¡å¤±è´¥: {len(failed)} ç¯‡")
        
        if failed:
            print(f"\nå¤±è´¥çš„è®ºæ–‡:")
            for fail_info in failed[:5]:
                print(f"  - {fail_info.get('title', 'Unknown')[:50]}")
                print(f"    ID: {fail_info.get('paper_id', 'N/A')}")
                print(f"    é”™è¯¯: {fail_info.get('error', 'Unknown')[:80]}")
            if len(failed) > 5:
                print(f"  ... è¿˜æœ‰ {len(failed)-5} ç¯‡ï¼ˆè¯¦è§ failed_papers.jsonï¼‰")
        
        # æ˜¾ç¤ºç¤ºä¾‹ï¼ˆä»æ–‡ä»¶è¯»å–æœ€åä¸€ä¸ªï¼‰
        if processed > 0 and output_file.exists():
            print("\n" + "=" * 80)
            print("ğŸ“ æ¨ç†é“¾ç¤ºä¾‹ï¼ˆæœ€åå¤„ç†çš„ä¸€ç¯‡ï¼‰")
            print("=" * 80)
            
            # è¯»å–æœ€åä¸€è¡Œ
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if lines:
                        example = json.loads(lines[-1].strip())
                        print(f"\nè®ºæ–‡: {example.get('title', 'N/A')[:60]}...")
                        print(f"DOI: {example.get('doi', 'N/A')}")
                        print(f"æœŸåˆŠ: {example.get('journal', 'N/A')}")
                        print(f"å¹´ä»½: {example.get('year', 'N/A')}")
                        
                        # æ‰å¹³åŒ–çš„æ¨ç†é“¾å­—æ®µ
                        print(f"\nğŸ¯ é—®é¢˜æ‹†è§£ (å‰200å­—ç¬¦):")
                        problem_text = example.get('problem_decomposition', 'N/A')
                        print(f"  {problem_text[:200]}...")
                        
                        print(f"\nğŸ“Š æ•°æ®éœ€æ±‚ (å‰200å­—ç¬¦):")
                        data_text = example.get('data', 'N/A')
                        print(f"  {data_text[:200]}...")
                        
                        print(f"\nğŸ”¬ ç ”ç©¶æ–¹æ³• (å‰200å­—ç¬¦):")
                        method_text = example.get('method', 'N/A')
                        print(f"  {method_text[:200]}...")
                        
                        print(f"\nâœ¨ ç»“è®º (å‰200å­—ç¬¦):")
                        conclusion_text = example.get('conclusion', 'N/A')
                        print(f"  {conclusion_text[:200]}...")
            except Exception as e:
                print(f"âš  æ— æ³•æ˜¾ç¤ºç¤ºä¾‹: {e}")
        
        print("\n" + "=" * 80)
        print("âœ… å®Œæˆï¼")
        print("=" * 80)
        
        if resume and papers_to_process:
            print("\nğŸ’¡ æç¤º:")
            print("  - å¦‚æœä¸­é€”ä¸­æ–­ï¼Œå†æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­")
            print("  - å·²å¤„ç†çš„è®ºæ–‡ä¼šè¢«è‡ªåŠ¨è·³è¿‡")
            print(f"  - è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        print("=" * 80)
        
        # ä¿å­˜æ—¥å¿—
        log_file = output_file.parent / "reasoning_extraction_log.json"
        log_data = {
            "total_papers": len(papers),
            "previously_processed": len(processed_ids),
            "this_batch_processed": processed,
            "cumulative_processed": total_processed,
            "this_batch_failed": len(failed),
            "resume_enabled": resume,
            "output_file": str(output_file),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import config
    
    parser = argparse.ArgumentParser(description="ç›´æ¥æå–ç§‘ç ”æ¨ç†é“¾")
    parser.add_argument(
        "--data-dir",
        type=str,
        default="./nature_neuroscience",
        help="æ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./data/reasoning_chains.jsonl",
        help="è¾“å‡ºæ–‡ä»¶"
    )
    parser.add_argument(
        "--max-papers",
        type=int,
        default=5,
        help="æœ€å¤šå¤„ç†å¤šå°‘ç¯‡è®ºæ–‡ï¼ˆ0=å…¨éƒ¨ï¼‰"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=config.OPENAI_API_KEY,
        help="APIå¯†é’¥"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=config.OPENAI_MODEL,
        help="æ¨¡å‹åç§°"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæå–å™¨
    extractor = DirectReasoningExtractor(
        api_key=args.api_key,
        model=args.model
    )
    
    # æ‰¹é‡å¤„ç†
    max_papers = None if args.max_papers == 0 else args.max_papers
    
    extractor.batch_process(
        data_dir=args.data_dir,
        output_file=args.output,
        max_papers=max_papers
    )


if __name__ == "__main__":
    main()

