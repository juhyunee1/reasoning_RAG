"""
ç›´æ¥ä»è®ºæ–‡æå–ç§‘ç ”æ¨ç†é“¾
ç”Ÿæˆreasoning_chains.jsonlæ ¼å¼
"""

import json
import uuid
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
from datetime import datetime
from tqdm import tqdm

class DirectReasoningExtractor:
    """ç›´æ¥æå–ç§‘ç ”æ¨ç†é“¾"""
    
    def __init__(self, api_key: str, model: str = "qwen3-max"):
        """åˆå§‹åŒ–æå–å™¨"""
        self.api_key = api_key
        self.model = model
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        # å…³é”®ç« èŠ‚ï¼ˆç”¨äºæå–æ¨ç†é“¾ï¼‰
        self.key_sections = ['abstract', 'introduction', 'main', 'results', 'discussion', 'conclusion', 'methods']
    
    def load_metadata(self, metadata_file: Path) -> Dict[str, Dict]:
        """
        åŠ è½½æ‰€æœ‰è®ºæ–‡çš„å…ƒæ•°æ®
        
        åŒæ—¶é€šè¿‡ id å’Œ doi å»ºç«‹ç´¢å¼•ï¼Œæ”¯æŒåŒé‡åŒ¹é…
        """
        print("åŠ è½½å…ƒæ•°æ®...")
        metadata_dict = {}
        metadata_dict_by_doi = {}
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–metadata"):
                if line.strip():
                    meta = json.loads(line)
                    # é€šè¿‡ id ç´¢å¼•
                    if 'id' in meta:
                        metadata_dict[meta['id']] = meta
                    # é€šè¿‡ doi ç´¢å¼•ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
                    if 'doi' in meta and meta['doi']:
                        metadata_dict_by_doi[meta['doi']] = meta
        
        print(f"âœ“ åŠ è½½äº† {len(metadata_dict)} ç¯‡è®ºæ–‡çš„å…ƒæ•°æ®")
        print(f"âœ“ å…¶ä¸­ {len(metadata_dict_by_doi)} ç¯‡æœ‰ DOI ä¿¡æ¯")
        
        # è¿”å›åŒé‡ç´¢å¼•å­—å…¸
        return {
            'by_id': metadata_dict,
            'by_doi': metadata_dict_by_doi
        }
    
    def load_media(self, media_file: Path) -> Dict[str, List]:
        """
        åŠ è½½æ‰€æœ‰è®ºæ–‡çš„åª’ä½“æ•°æ®ï¼ˆfigures, tablesç­‰ï¼‰
        
        æ³¨æ„ï¼šmedia.jsonlä¸­æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå›¾è¡¨ï¼ŒåŒä¸€ç¯‡è®ºæ–‡æœ‰å¤šè¡Œ
        éœ€è¦æŒ‰paper_idæˆ–doièšåˆï¼Œæ”¯æŒåŒé‡åŒ¹é…
        """
        print("åŠ è½½åª’ä½“æ•°æ®...")
        media_dict_by_id = {}
        media_dict_by_doi = {}
        
        with open(media_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–media"):
                if line.strip():
                    media_item = json.loads(line)
                    
                    # æå–å…³é”®ä¿¡æ¯
                    media_info = {
                        'type': media_item.get('type'),
                        'label': media_item.get('label'),
                        'caption': media_item.get('caption'),
                        'legend': media_item.get('legend'),
                        'name': media_item.get('name')
                    }
                    
                    # é€šè¿‡ id ç´¢å¼•
                    paper_id = media_item.get('id')
                    if paper_id:
                        if paper_id not in media_dict_by_id:
                            media_dict_by_id[paper_id] = []
                        media_dict_by_id[paper_id].append(media_info)
                    
                    # é€šè¿‡ doi ç´¢å¼•ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
                    paper_doi = media_item.get('doi')
                    if paper_doi:
                        if paper_doi not in media_dict_by_doi:
                            media_dict_by_doi[paper_doi] = []
                        media_dict_by_doi[paper_doi].append(media_info)
        
        total_papers_by_id = len(media_dict_by_id)
        total_papers_by_doi = len(media_dict_by_doi)
        total_media = sum(len(items) for items in media_dict_by_id.values())
        
        print(f"âœ“ é€šè¿‡ ID ç´¢å¼•: {total_papers_by_id} ç¯‡è®ºæ–‡")
        print(f"âœ“ é€šè¿‡ DOI ç´¢å¼•: {total_papers_by_doi} ç¯‡è®ºæ–‡")
        print(f"âœ“ æ€»è®¡ {total_media} ä¸ªå›¾è¡¨")
        
        # è¿”å›åŒé‡ç´¢å¼•å­—å…¸
        return {
            'by_id': media_dict_by_id,
            'by_doi': media_dict_by_doi
        }
    
    def parse_contents_file(self, contents_file: Path, max_papers: int = None) -> List[Dict]:
        """
        è§£æcontents.jsonlæ–‡ä»¶ï¼ˆä½¿ç”¨çŠ¶æ€æœºå¤„ç†å­—ç¬¦ä¸²ä¸­çš„æ‹¬å·ï¼‰
        """
        print(f"\nè§£æcontentsæ–‡ä»¶...")
        papers = []
        
        with open(contents_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        pos = 0
        paper_count = 0
        failed_count = 0
        
        with tqdm(desc="è§£æè®ºæ–‡") as pbar:
            while pos < len(content):
                # è·³è¿‡ç©ºç™½å­—ç¬¦
                while pos < len(content) and content[pos] in ' \t\n\r':
                    pos += 1
                
                if pos >= len(content):
                    break
                
                # æ‰¾åˆ°JSONå¯¹è±¡çš„èµ·å§‹ä½ç½®
                if content[pos] != '{':
                    pos += 1
                    continue
                
                start = pos
                brace_count = 0
                in_string = False
                escape = False
                
                # çŠ¶æ€æœºï¼šæ­£ç¡®å¤„ç†å­—ç¬¦ä¸²ä¸­çš„æ‹¬å·
                for i in range(start, len(content)):
                    char = content[i]
                    
                    if escape:
                        escape = False
                        continue
                    
                    if char == '\\':
                        escape = True
                        continue
                    
                    if char == '"':
                        in_string = not in_string
                        continue
                    
                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end = i + 1
                                
                                try:
                                    paper = json.loads(content[start:end])
                                    papers.append(paper)
                                    paper_count += 1
                                    pbar.update(1)
                                    
                                    if max_papers and paper_count >= max_papers:
                                        print(f"âœ“ å·²è¾¾åˆ°è®¾å®šçš„æ•°é‡é™åˆ¶: {max_papers} ç¯‡")
                                        print(f"âœ“ æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
                                        return papers
                                        
                                except json.JSONDecodeError as e:
                                    failed_count += 1
                                    if failed_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                                        tqdm.write(f"âš  è·³è¿‡è§£æå¤±è´¥çš„å¯¹è±¡ #{paper_count+failed_count}: {str(e)[:80]}")
                                
                                pos = end
                                break
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·ï¼Œè¯´æ˜æ–‡ä»¶ç»“æŸæˆ–æ ¼å¼é”™è¯¯
                    break
        
        if failed_count > 0:
            print(f"âš  å…±æœ‰ {failed_count} ä¸ªå¯¹è±¡è§£æå¤±è´¥ï¼ˆå·²è·³è¿‡ï¼‰")
        print(f"âœ“ æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def extract_key_content(self, paper_content: Dict, media_data: List = None, max_total_chars: int = None, is_review: bool = False) -> str:
        """
        æå–è®ºæ–‡çš„å®Œæ•´å…³é”®å†…å®¹ç”¨äºæ¨ç†é“¾åˆ†æ
        
        ç­–ç•¥ï¼š
        - ç ”ç©¶è®ºæ–‡æ¨¡å¼ï¼šæå–æ‰€æœ‰å…³é”®ç« èŠ‚çš„å®Œæ•´å†…å®¹ï¼ˆintroduction, main, methods, results, discussion, conclusionï¼‰
        - ç»¼è¿°æ¨¡å¼ï¼šä¿ç•™æ‰€æœ‰æ­£æ–‡å†…å®¹ï¼Œæ’é™¤Referencesã€Rights And Permissionsã€Author Informationç­‰éƒ¨åˆ†
        
        Args:
            paper_content: è®ºæ–‡å†…å®¹
            media_data: åª’ä½“æ•°æ®ï¼ˆfigures, tablesç­‰ï¼‰
            max_total_chars: æœ€å¤§å­—ç¬¦æ•°ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
            is_review: æ˜¯å¦æ˜¯ç»¼è¿°è®ºæ–‡ï¼ˆé»˜è®¤Falseï¼‰
        Returns:
            å…³é”®å†…å®¹çš„å®Œæ•´æ–‡æœ¬
        """
        sections_list = paper_content.get('sections', [])
        
        # ç»¼è¿°æ¨¡å¼ï¼šä¿ç•™æ‰€æœ‰æ­£æ–‡å†…å®¹ï¼Œæ’é™¤ä¸éœ€è¦çš„ç« èŠ‚
        if is_review:
            # éœ€è¦æ’é™¤çš„ç« èŠ‚å…³é”®è¯
            exclude_keywords = [
                'references', 'reference',
                'rights and permissions', 'rights & permissions',
                'author information', 'author contributions', 'authors',
                'acknowledgments', 'acknowledgement',
                'supplementary', 'supplemental',
                'data availability', 'code availability',
                'competing interests', 'conflict of interest',
                'ethics', 'ethical',
                'peer review', 'reviewer',
                'publisher', 'copyright',
                'about this article', 'about the article',
                'article history', 'publication history'
            ]
            
            final_parts = []
            
            # éå†æ‰€æœ‰ç« èŠ‚ï¼Œæ’é™¤ä¸éœ€è¦çš„ç« èŠ‚
            for section in sections_list:
                section_title = section.get('section_title', '')
                section_title_lower = section_title.lower()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤
                should_exclude = False
                for keyword in exclude_keywords:
                    if keyword in section_title_lower:
                        should_exclude = True
                        break
                
                if should_exclude:
                    continue
                
                # æå–ç« èŠ‚å†…å®¹
                section_texts = section.get('section_text', [])
                section_content = []
                
                for text_item in section_texts:
                    if isinstance(text_item, dict):
                        text = text_item.get('content', '')
                    else:
                        text = str(text_item)
                    
                    text = text.strip()
                    if text:
                        section_content.append(text)
                
                if section_content:
                    final_parts.append(f"\n## {section_title}:\n" + "\n".join(section_content))
            
            # æ·»åŠ å›¾è¡¨ä¿¡æ¯
            if media_data:
                figure_captions = []
                for media_item in media_data:
                    media_type = media_item.get('type', '').lower()
                    caption = media_item.get('caption', '')
                    legend = media_item.get('legend', '')
                    name = media_item.get('name', '')
                    
                    if caption or legend or name:
                        caption_text = f"{media_type.upper()}: "
                        if name:
                            caption_text += f"{name}. "
                        if caption:
                            caption_text += caption
                        if legend:
                            caption_text += f" [Legend: {legend}]"
                        figure_captions.append(caption_text.strip())
                
                if figure_captions:
                    final_parts.append(f"\n## Figures and Tables:\n" + "\n".join(figure_captions))
            
            return "\n".join(final_parts)
        
        # ç ”ç©¶è®ºæ–‡æ¨¡å¼ï¼šæå–å…³é”®ç« èŠ‚
        # å…³é”®ç« èŠ‚åˆ—è¡¨
        key_section_keys = ['introduction', 'main', 'methods', 'results', 'discussion', 'conclusion']
        
        # ä¸åŒæœŸåˆŠçš„ç« èŠ‚å‘½åè§„èŒƒï¼š
        # - nature: Abstract, Main, Discussion, Methods, æ—  Results
        # - nature neuroscience: Abstract, Main, Results, Discussion, Methods
        # - nature communication: Abstract, Introduction, Results, Discussion, Methods 
        # - neuron: Summary, Keywords, Introduction, Results, Discussion, Experimental Procedures
        # - cell: Summary, Kerwords, Introduction, Results, Discussion, Experimental Procedures
        # - nature reviews neuroscience: Abstract, Main, Results, Discussion, Methods
        # - science: Abstract, No Results, Discussion
        
        # Methods ç« èŠ‚çš„å„ç§å‘½åï¼ˆéœ€è¦ç‰¹åˆ«å¤„ç†ï¼‰
        methods_keywords = ['methods', 'method', 'experimental procedures', 'experimental procedure', 
                           'materials and methods', 'materials & methods', 'procedures']
        
        sections_dict = {}
        
        # ç‰¹æ®Šå¤„ç†ï¼šNature æœŸåˆŠæ²¡æœ‰ Results æ ‡é¢˜ï¼Œè€Œæ˜¯ç”¨ Main å’Œ Discussion ä¹‹é—´çš„ç« èŠ‚ä½œä¸º Results
        # å…ˆæ‰¾å‡º Main å’Œ Discussion çš„ä½ç½®
        main_idx = None
        discussion_idx = None
        
        for idx, section in enumerate(sections_list):
            section_title = section.get('section_title', '')
            section_title_lower = section_title.lower()
            
            # è¯†åˆ« Mainï¼ˆæ³¨æ„ï¼šMain æ ‡é¢˜é€šå¸¸å¾ˆçŸ­ï¼Œé¿å…è¯¯åŒ¹é…åŒ…å«"main"çš„å…¶ä»–æ ‡é¢˜ï¼‰
            if 'main' in section_title_lower and len(section_title) < 20:
                main_idx = idx
            # è¯†åˆ« Discussion
            if 'discussion' in section_title_lower:
                discussion_idx = idx
        
        # æå– Main å’Œ Discussion ä¹‹é—´çš„ç« èŠ‚ä½œä¸º Resultsï¼ˆä»…å½“ä¸¤è€…éƒ½å­˜åœ¨ä¸”ä¹‹é—´æœ‰ç« èŠ‚æ—¶ï¼‰
        nature_results_sections = []
        if main_idx is not None and discussion_idx is not None and discussion_idx > main_idx + 1:
            for idx in range(main_idx + 1, discussion_idx):
                nature_results_sections.append(idx)
        
        # éå†æ‰€æœ‰ç« èŠ‚è¿›è¡ŒåŒ¹é…
        for idx, section in enumerate(sections_list):
            section_title = section.get('section_title', '')
            section_title_lower = section_title.lower()
            section_texts = section.get('section_text', [])
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®ç« èŠ‚
            matched_key = None
            
            # ç‰¹æ®Šå¤„ç† 1: å¦‚æœæ˜¯ Nature çš„ Results ç« èŠ‚ï¼ˆMain å’Œ Discussion ä¹‹é—´ï¼‰
            if idx in nature_results_sections:
                matched_key = 'results'
            # ç‰¹æ®Šå¤„ç† 2: å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ Methods ç›¸å…³ç« èŠ‚
            # Neuron ä½¿ç”¨ "Experimental Procedures"ï¼Œéœ€è¦åŒ¹é…åˆ° methods
            elif any(keyword in section_title_lower for keyword in methods_keywords):
                matched_key = 'methods'
            # ç‰¹æ®Šå¤„ç† 3: å…¶ä»–ç« èŠ‚çš„åŒ¹é…
            else:
                for key in key_section_keys:
                    if key in section_title_lower:
                        matched_key = key
                        break
            
            if matched_key:
                section_content = []
                
                # å®Œæ•´æå–æ‰€æœ‰å†…å®¹ï¼Œä¸æˆªæ–­
                for text_item in section_texts:
                    if isinstance(text_item, dict):
                        text = text_item.get('content', '')
                    else:
                        text = str(text_item)
                    
                    text = text.strip()
                    if text:
                        section_content.append(text)
                
                if section_content:
                    # æ ¹æ®ç« èŠ‚ç±»å‹æ·»åŠ æ ‡è®°
                    if matched_key in ['introduction', 'main']:
                        label = "[IMPORTANT FOR PROBLEM ANALYSIS]"
                    elif matched_key == 'methods':
                        label = "[IMPORTANT FOR DATA & METHODS]"
                    elif matched_key == 'results':
                        label = "[IMPORTANT FOR CONCLUSION - FINDINGS]"
                    elif matched_key == 'discussion':
                        label = "[IMPORTANT FOR CONCLUSION - SIGNIFICANCE]"
                    else:
                        label = ""
                    
                    # å¯¹äº Resultsï¼Œå¦‚æœå·²æœ‰å†…å®¹ï¼Œåˆ™è¿½åŠ ï¼ˆNature æœŸåˆŠå¯èƒ½æœ‰å¤šä¸ª Results ç« èŠ‚ï¼‰
                    if matched_key == 'results' and matched_key in sections_dict:
                        # è¿½åŠ åˆ°å·²æœ‰çš„ Results å†…å®¹
                        existing_content = sections_dict[matched_key]
                        new_content = f"\n## {section_title}:\n" + "\n".join(section_content)
                        sections_dict[matched_key] = existing_content + "\n" + new_content
                    else:
                        sections_dict[matched_key] = f"\n## {section_title} {label}:\n" + "\n".join(section_content)
        
        # æå–Figureså’ŒTablesçš„captionsï¼ˆå®Œæ•´æå–ï¼Œæ— æ•°é‡é™åˆ¶ï¼‰
        figures_content = ""
        if media_data:
            figure_captions = []
            # æå–æ‰€æœ‰å›¾è¡¨ï¼Œä¸å†é™åˆ¶æ•°é‡
            for media_item in media_data:
                media_type = media_item.get('type', '').lower()
                caption = media_item.get('caption', '')
                legend = media_item.get('legend', '')
                name = media_item.get('name', '')
                
                if caption or legend or name:
                    caption_text = f"{media_type.upper()}: "
                    if name:
                        caption_text += f"{name}. "
                    if caption:
                        caption_text += caption
                    if legend:
                        caption_text += f" [Legend: {legend}]"
                    figure_captions.append(caption_text.strip())
            
            if figure_captions:
                figures_content = f"\n## Figures and Tables [IMPORTANT FOR DATA & METHODS]:\n" + "\n".join(figure_captions)
        
        # æŒ‰ä¼˜å…ˆé¡ºåºç»„è£…å†…å®¹ï¼šIntroduction â†’ Methods â†’ Figures â†’ Results â†’ Discussion
        final_parts = []
        
        # 1. Introduction/Main
        if 'introduction' in sections_dict:
            final_parts.append(sections_dict['introduction'])
        elif 'main' in sections_dict:
            final_parts.append(sections_dict['main'])
        
        # 2. Methods
        if 'methods' in sections_dict:
            final_parts.append(sections_dict['methods'])
        
        # 3. Figuresï¼ˆæ’å…¥åˆ°Methodså’ŒResultsä¹‹é—´ï¼‰
        if figures_content:
            final_parts.append(figures_content)
        
        # 4. Resultsï¼ˆæœ€é‡è¦ï¼Œå®Œæ•´ä¿ç•™ï¼‰
        if 'results' in sections_dict:
            final_parts.append(sections_dict['results'])
        
        # 5. Discussionï¼ˆæœ€é‡è¦ï¼Œå®Œæ•´ä¿ç•™ï¼‰
        if 'discussion' in sections_dict:
            final_parts.append(sections_dict['discussion'])
        
        return "\n".join(final_parts)
    
    def build_reasoning_prompt(self, paper_title: str, key_content: str) -> str:
        prompt = f"""You are an expert in scientific reasoning and research methodology in neuroscience. Your task is to extract the CORE SCIENTIFIC REASONING CHAIN from this paper in a CONCISE, STREAMLINED format.

Paper Title
{paper_title}

Key Content from Paper
{key_content}  

Your Task
Extract the CORE reasoning logic in 4 CONCISE PARAGRAPHS. Focus on the ESSENCE, not details.

1. problem_decomposition: Core Research Question and Its Logical Breakdown
Read the Introduction/Main section strategically (early â†’ middle â†’ late paragraphs) and write ONE CONCISE PARAGRAPH  that captures:
- Early paragraphs: The broad background context and macroscopic problem
- Middle paragraphs: The mechanistic/phenotypic gap or unresolved question
- Late paragraphs: The specific hypothesis or core research question this paper addresses
Synthesize these three layers into a coherent logical flow: background â†’ gap â†’ hypothesis.

2. data: Data Sources and Requirements
Read the Methods section and Figure/Table captions and write ONE CONCISE PARAGRAPH  describing what data this research depends on. Include:
- Sample source: What subjects/models were used (species, age, strain, sample size)
- Data type: What was measured (neural recordings, behavior, imaging, molecular data)
- Sampling characteristics: Recording methods, temporal/spatial resolution, duration and brain regions
- Task conditions: What behavioral paradigms or experimental manipulations were applied, describe the structure of the behavior task(most important part)
Use natural, flowing language to describe the data foundation of this study.

3. method: Experimental Design and Data Acquisition Methods
Read the Methods, Figure captions, and Results sections and write ONE CONCISE PARAGRAPH  describing HOW the required data were obtained. Include:
- Experimental design: Control vs. experimental groups, within/between-subject design, sample size rationale
- Data acquisition methods: Specific techniques and instruments used to collect each data type (e.g., recording setup, behavioral tracking, imaging parameters)
- Experimental conditions: Manipulated variables, control conditions, timing/sequence of interventions
- Analytical pipeline: Key processing and analysis steps linking raw data to testable predictions
Focus on the DESIGN LOGIC: what methods were chosen to obtain what data, and how the experimental design allows testing the hypothesis.

4. conclusion: Key Findings, Answer, and Significance
Read the Results and Discussion sections and write ONE CONCISE PARAGRAPH that:
- States how these experiments and data analysis results lead to the findings
- Summarizes the logical chain: Data -> Method -> Conclusion -> Scientific findings(most important part)

Critical Instructions:
- In Nature Neuroscience papers, the "Main" section IS the Introduction
- Each field should be ONE CONCISE PARAGRAPH (not lists, not multiple paragraphs)
- Focus on CORE REASONING LOGIC, not exhaustive details
- Write in natural, flowing language suitable for embedding models
- Output in English

Output Format (JSON):
{{
    "problem_decomposition": "...",
    "data": "...",
    "method": "...",
    "conclusion": "..."
}}

Example Output:
{{
    "problem_decomposition": "Synapse development requires coordinated assembly of pre- and postsynaptic components at precise subcellular locations. While neuroligin-neurexin complexes are established synaptogenic pairs, the diversity of synaptic connections suggests additional trans-synaptic adhesion systems remain to be discovered. The mechanistic gap lies in identifying novel heterophilic receptor-ligand pairs that not only mediate cell-cell adhesion but also recruit intracellular scaffolds like PSD-95 to organize functional synapses. This leads to the hypothesis that NGL (netrin-G ligand) proteins, as postsynaptic adhesion molecules binding both presynaptic netrin-G and postsynaptic PSD-95, constitute a bidirectional synaptogenic system regulating excitatory synapse formation.",
    "data": "The study used cultured rat hippocampal neurons (embryonic day 18) and transfected HEK293T cells as experimental models, with n=30-50 neurons per condition across three independent cultures. Protein localization was assessed via confocal immunofluorescence and postembedding immunoelectron microscopy targeting synaptic markers (PSD-95, synapsin I, VGlut1). Functional measurements included whole-cell patch-clamp recordings of miniature excitatory postsynaptic currents (mEPSCs) to quantify synapse number and strength. Key manipulations included lentiviral overexpression, siRNA-mediated knockdown of endogenous NGL-2, and synaptogenic bead assays where NGL-coated microspheres were applied to axons to test presynaptic differentiation capacity.",
    "method": "The research employed a multi-level experimental strategy integrating molecular, structural, and functional analyses. NGL-2 was first identified as a PSD-95 interactor via yeast two-hybrid screening and validated by coimmunoprecipitation from brain lysates. To test synaptogenic function, neurons were transfected with NGL-2 or control vectors at DIV7 and analyzed at DIV14 for changes in synaptic puncta density and mEPSC frequency. Loss-of-function experiments used lentiviral shRNA to knock down endogenous NGL-2 from DIV5-14, followed by blind quantification of dendritic spine density and electrophysiology. Cell-surface presentation assays tested sufficiency: NGL-2-expressing HEK293 cells or antibody-coated beads were cocultured with neurons to induce presynaptic differentiation visualized by synapsin clustering. Statistical comparisons used one-way ANOVA with post-hoc Tukey tests.",
    "conclusion": "NGL-2 localizes specifically to excitatory postsynaptic sites and mediates trans-synaptic adhesion by binding presynaptic netrin-G2 in an isoform-specific manner while simultaneously recruiting PSD-95 via its C-terminal PDZ-binding motif. Overexpression increased excitatory synapse density and mEPSC frequency by 40%, whereas siRNA knockdown reduced synapse number by 35% without affecting inhibitory synapses, demonstrating selective regulation of excitatory synaptogenesis. Application of soluble NGL-2 ectodomain competitively disrupted existing synapses, confirming its necessity for synapse maintenance. These findings establish the NGL-netrin-G complex as a novel trans-synaptic organizing system parallel to neurexin-neuroligin, revealing molecular diversity in synapse specification with implications for understanding circuit wiring, synaptic plasticity, and neurodevelopmental disorders like autism spectrum disorders."
}}

Return ONLY the JSON object with 4 fields, no additional text.
"""
        return prompt
    
    def build_survey_prompt(self, paper_title: str, key_content: str) -> str:
        """æ„å»ºç»¼è¿°è®ºæ–‡çš„æ¨ç†é“¾æå–prompt"""
        prompt = f"""You are an expert in scientific reasoning and research methodology in neuroscience. Your task is to extract the CORE SCIENTIFIC REASONING CHAIN from this REVIEW/SURVEY paper in a CONCISE, STREAMLINED format.

Paper Title
{paper_title}

Key Content from Paper
{key_content}  

Your Task
Extract the CORE reasoning logic in 4 CONCISE PARAGRAPHS. Focus on the ESSENCE of how the review synthesizes the field, not details.

1. problem_decomposition: Field Problem Landscape and Key Questions
Read the Abstract and Introduction strategically (early â†’ mid â†’ late paragraphs) and write ONE CONCISE PARAGRAPH that captures:
- Early paragraphs: Broad neuroscientific context and overarching domain question
- Middle paragraphs: Key mechanistic unknowns, theoretical debates, or unresolved gaps
- Late paragraphs: Specific sub-questions or frameworks the review is organized around
Focus on the problem landscape of the field, not a single experimental hypothesis.

2. evidence: Empirical Foundation Integrated by the Review
Read early-to-mid Main and figure captions focusing on empirical studies and write ONE CONCISE PARAGRAPH describing the empirical foundation this review integrates:
- Species/systems studied (human, rodent, primate, cell prep, computational models, etc.)
- Key data modalities (electrophysiology, imaging, behavior, genetics, computational modeling, clinical studies)
- Major experimental paradigms and brain regions represented
- Cross-scale linkage (molecular â†’ circuit â†’ systems â†’ behavior)
Emphasize what empirical sources the field uses, not specific datasets from this paper.

3. framework: Knowledge Organization and Synthesis Strategy
Read late Main, figure captions, and Discussion and write ONE CONCISE PARAGRAPH describing how the review organizes and synthesizes knowledge:
- Theoretical or mechanistic frameworks compared or integrated
- Competing/alternative models and how the review reconciles them
- Cross-study synthesis strategy (causal chain, computation-to-circuit mapping, cross-species convergence, etc.)
- Key conceptual diagrams, schemas, or explanatory mechanisms emphasized
Focus on the logic that structures the field and how the review creates coherence across diverse findings.

4. conclusion: Field Consensus, Gaps, and Future Directions
Read the Conclusion / Discussion and write ONE PARAGRAPH that:
- States current consensus or leading mechanistic view(s)
- Summarizes major open questions and controversies
- Identifies methodological or conceptual limitations
- Highlights future research directions or proposed experimental/theoretical roadmaps
Summarize the logical path: evidence â†’ synthesis â†’ field status â†’ what's next

Critical Instructions:
- This is a REVIEW/SURVEY paper, focus on field-level synthesis, not single experiments
- Each field should be ONE CONCISE PARAGRAPH (not lists, not multiple paragraphs)
- Focus on CORE REASONING LOGIC and field-level insights
- Write in natural, flowing language suitable for embedding models
- Output in English

Output Format (JSON):
{{
    "problem_decomposition": "...",
    "evidence": "...",
    "framework": "...",
    "conclusion": "..."
}}

Return ONLY the JSON object with 4 fields, no additional text.
"""
        return prompt
    
    def extract_reasoning_chain(self, paper_content: Dict, metadata: Dict, media_data: List = None, is_review: bool = False) -> Optional[Dict]:
        """
        ä»å•ç¯‡è®ºæ–‡ç›´æ¥æå–æ¨ç†é“¾
        
        Args:
            paper_content: è®ºæ–‡å†…å®¹
            metadata: è®ºæ–‡å…ƒæ•°æ®
            media_data: åª’ä½“æ•°æ®ï¼ˆfigures, tablesç­‰ï¼‰
            is_review: æ˜¯å¦ä½¿ç”¨ç»¼è¿°æå–æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨ç ”ç©¶è®ºæ–‡æ¨¡å¼ï¼‰
            
        Returns:
            æ¨ç†é“¾æ•°æ®ï¼ˆåŒ…å«research_reasoningï¼‰
        """
        paper_title = paper_content.get('title', 'Unknown')
        
        # æå–å…³é”®å†…å®¹ï¼ˆåŒ…æ‹¬media captionsï¼‰
        key_content = self.extract_key_content(paper_content, media_data, is_review=is_review)
                
        # æ˜¾ç¤ºå†…å®¹é•¿åº¦ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¼‰
        content_length_k = len(key_content) / 1000
        print(f"  ğŸ“„ å†…å®¹é•¿åº¦: {content_length_k:.1f}K å­—ç¬¦")
        
        # æ ¹æ®å‚æ•°é€‰æ‹©æå–æ¨¡å¼
        if is_review:
            print(f"  ğŸ“š ä½¿ç”¨ç»¼è¿°æå–æ¨¡å¼")
            # æ„å»ºç»¼è¿°Prompt
            prompt = self.build_survey_prompt(paper_title, key_content)
            required_fields = ['problem_decomposition', 'evidence', 'framework', 'conclusion']
        else:
            print(f"  ğŸ”¬ ä½¿ç”¨ç ”ç©¶è®ºæ–‡æå–æ¨¡å¼")
            # æ„å»ºç ”ç©¶è®ºæ–‡Prompt
            prompt = self.build_reasoning_prompt(paper_title, key_content)
            required_fields = ['problem_decomposition', 'data', 'method', 'conclusion']
        
        # è°ƒç”¨LLMæå–æ¨ç†é“¾
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are an expert in scientific reasoning extraction. Always respond with valid JSON."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
                
                reasoning = json.loads(response.choices[0].message.content)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if not all(field in reasoning for field in required_fields):
                    raise ValueError(f"Missing required fields in LLM response")
                
                # æ„å»ºå®Œæ•´æ•°æ®ï¼ˆLLM ç›´æ¥è¿”å›æ®µè½æ ¼å¼ï¼‰
                result = {
                    "paper_id": paper_content.get('id'),
                    "doi": paper_content.get('doi'),
                    "title": paper_title,
                    "journal": metadata.get('journal'),
                    "year": metadata.get('publish_year', '').split('/')[0] if metadata.get('publish_year') else '',
                    "citation_count": metadata.get('citation_count'),
                    "is_open_access": metadata.get('is_open_access'),
                    "authors": metadata.get('authors', []),
                    "article_url": metadata.get('article_url'),
                    
                    # æ ¸å¿ƒæ¨ç†é“¾ï¼ˆç²¾ç‚¼çš„æ®µè½æ ¼å¼ï¼‰
                    "problem_decomposition": reasoning['problem_decomposition'],
                    "conclusion": reasoning['conclusion']
                }
                
                # æ ¹æ®è®ºæ–‡ç±»å‹æ·»åŠ ä¸åŒçš„å­—æ®µ
                if is_review:
                    # ç»¼è¿°è®ºæ–‡ï¼šä½¿ç”¨ evidence å’Œ framework
                    result["evidence"] = reasoning['evidence']
                    result["framework"] = reasoning['framework']
                else:
                    # ç ”ç©¶è®ºæ–‡ï¼šä½¿ç”¨ data å’Œ method
                    result["data"] = reasoning['data']
                    result["method"] = reasoning['method']
                
                return result
                
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"  âœ— æå–å¤±è´¥: {e}")
                    return None
        
        return None
    
    def _match_paper_data(
        self,
        paper: Dict,
        metadata_dict: Dict,
        media_dict: Dict
    ) -> Tuple[Dict, List]:
        """
        åŒ¹é…è®ºæ–‡çš„å…ƒæ•°æ®å’Œåª’ä½“æ•°æ®
        
        ä¼˜å…ˆä½¿ç”¨ id åŒ¹é…ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ doi åŒ¹é…
        
        Args:
            paper: è®ºæ–‡å†…å®¹å­—å…¸
            metadata_dict: å…ƒæ•°æ®å­—å…¸ï¼ˆåŒ…å« by_id å’Œ by_doiï¼‰
            media_dict: åª’ä½“æ•°æ®å­—å…¸ï¼ˆåŒ…å« by_id å’Œ by_doiï¼‰
            
        Returns:
            (metadata, media_data) å…ƒç»„
        """
        paper_id = paper.get('id')
        paper_doi = paper.get('doi')
        
        # ä¼˜å…ˆé€šè¿‡ id åŒ¹é…å…ƒæ•°æ®
        metadata = {}
        if paper_id and 'by_id' in metadata_dict:
            metadata = metadata_dict['by_id'].get(paper_id, {})
        
        # å¦‚æœ id åŒ¹é…å¤±è´¥ï¼Œå°è¯•é€šè¿‡ doi åŒ¹é…
        if not metadata and paper_doi and 'by_doi' in metadata_dict:
            metadata = metadata_dict['by_doi'].get(paper_doi, {})
        
        # ä¼˜å…ˆé€šè¿‡ id åŒ¹é…åª’ä½“æ•°æ®
        media_data = []
        if paper_id and 'by_id' in media_dict:
            media_data = media_dict['by_id'].get(paper_id, [])
        
        # å¦‚æœ id åŒ¹é…å¤±è´¥ï¼Œå°è¯•é€šè¿‡ doi åŒ¹é…
        if not media_data and paper_doi and 'by_doi' in media_dict:
            media_data = media_dict['by_doi'].get(paper_doi, [])
        
        return metadata, media_data
    
    def _load_processed_papers(self, output_file: Path) -> set:
        """
        ä»è¾“å‡ºæ–‡ä»¶ä¸­åŠ è½½å·²å¤„ç†çš„è®ºæ–‡ID
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å·²å¤„ç†çš„paper_idé›†åˆ
        """
        processed_ids = set()
        if output_file.exists():
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            if 'paper_id' in data:
                                processed_ids.add(data['paper_id'])
                        except:
                            continue
            except Exception as e:
                print(f"âš  è­¦å‘Š: è¯»å–å·²æœ‰è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")
        
        return processed_ids
    
    def _save_single_result(self, result: Dict, output_file: Path):
        """
        å¢é‡ä¿å­˜å•ä¸ªç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        
        Args:
            result: å•ä¸ªè®ºæ–‡çš„æ¨ç†é“¾
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    def _save_failed_log(self, failed_list: list, output_file: Path):
        """
        ä¿å­˜å¤±è´¥æ—¥å¿—
        
        Args:
            failed_list: å¤±è´¥çš„è®ºæ–‡åˆ—è¡¨ [{paper_id, title, error}, ...]
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        log_file = output_file.parent / "failed_papers.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump({
                "total_failed": len(failed_list),
                "failed_papers": failed_list,
                "timestamp": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
    
    def batch_process(
        self,
        data_dir: Path,
        output_file: Path,
        max_papers: int = None,
        resume: bool = True,
        is_review: bool = False
    ):
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            output_file: è¾“å‡ºæ–‡ä»¶ï¼ˆreasoning_chains.jsonlï¼‰
            max_papers: æœ€å¤šå¤„ç†å¤šå°‘ç¯‡
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤Trueï¼‰
            is_review: æ˜¯å¦ä½¿ç”¨ç»¼è¿°æå–æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨ç ”ç©¶è®ºæ–‡æ¨¡å¼ï¼‰
        """
        data_dir = Path(data_dir)
        output_file = Path(output_file)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        print("=" * 80)
        print("ğŸ§  æå–ç§‘ç ”æ¨ç†é“¾ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
        print("=" * 80)
        if is_review:
            print("ğŸ“š æ¨¡å¼: ç»¼è¿°æå–æ¨¡å¼")
        else:
            print("ğŸ”¬ æ¨¡å¼: ç ”ç©¶è®ºæ–‡æå–æ¨¡å¼")
        print("=" * 80)
        
        # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
        processed_ids = set()
        if resume:
            processed_ids = self._load_processed_papers(output_file)
            if processed_ids:
                print(f"âœ“ å‘ç°å·²å¤„ç†çš„è®ºæ–‡: {len(processed_ids)} ç¯‡")
                print(f"âœ“ å°†è·³è¿‡å·²å¤„ç†çš„è®ºæ–‡ï¼Œä»æ–­ç‚¹ç»§ç»­")
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_dict = self.load_metadata(data_dir / "metadata.jsonl")
        
        # åŠ è½½åª’ä½“æ•°æ®
        media_dict = {}
        media_file = data_dir / "media.jsonl"
        if media_file.exists():
            media_dict = self.load_media(media_file)
        else:
            print("âš  è­¦å‘Š: æœªæ‰¾åˆ°media.jsonlï¼Œå°†ä¸æå–å›¾è¡¨ä¿¡æ¯")
            media_dict = {'by_id': {}, 'by_doi': {}}
        
        # è§£æcontents
        papers = self.parse_contents_file(
            data_dir / "contents.jsonl",
            max_papers=max_papers
        )
        
        if not papers:
            print("âœ— é”™è¯¯: æœªæ‰¾åˆ°è®ºæ–‡æ•°æ®")
            return
        
        # è¿‡æ»¤å·²å¤„ç†çš„è®ºæ–‡
        if resume and processed_ids:
            papers_to_process = [p for p in papers if p.get('id') not in processed_ids]
            print(f"\nâœ“ æ€»è®ºæ–‡æ•°: {len(papers)} ç¯‡")
            print(f"âœ“ å·²å¤„ç†: {len(processed_ids)} ç¯‡")
            print(f"âœ“ å¾…å¤„ç†: {len(papers_to_process)} ç¯‡")
        else:
            papers_to_process = papers
            print(f"\nâœ“ å°†å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡")
        
        if not papers_to_process:
            print("\nâœ“ æ‰€æœ‰è®ºæ–‡å·²å¤„ç†å®Œæˆï¼")
            return
        
        print("=" * 80)
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡ï¼ˆå¢é‡ä¿å­˜ï¼‰
        processed = 0
        failed = []
        
        for paper in tqdm(papers_to_process, desc="æå–è¿›åº¦"):
            try:
                paper_id = paper.get('id')
                paper_title = paper.get('title', 'Unknown')
                
                # ä½¿ç”¨åŒé‡åŒ¹é…è·å–å…ƒæ•°æ®å’Œåª’ä½“æ•°æ®
                metadata, media_data = self._match_paper_data(
                    paper, metadata_dict, media_dict
                )
                
                print(f"\nå¤„ç†: {paper_title[:60]}...")
                if not metadata:
                    print(f"  âš  è­¦å‘Š: æœªæ‰¾åˆ°åŒ¹é…çš„å…ƒæ•°æ® (ID: {paper_id})")
                if media_data:
                    print(f"  æ‰¾åˆ° {len(media_data)} ä¸ªå›¾è¡¨")
                
                # æå–æ¨ç†é“¾
                result = self.extract_reasoning_chain(paper, metadata, media_data, is_review=is_review)
                
                if result:
                    # ç«‹å³ä¿å­˜ï¼ˆå¢é‡å†™å…¥ï¼‰
                    self._save_single_result(result, output_file)
                    print(f"  âœ“ æå–æˆåŠŸå¹¶å·²ä¿å­˜")
                    processed += 1
                else:
                    failed.append({
                        "paper_id": paper_id,
                        "title": paper_title,
                        "error": "æå–å¤±è´¥"
                    })
                
            except Exception as e:
                error_msg = str(e)
                print(f"\nâœ— å¤„ç†å¤±è´¥: {paper.get('title', 'Unknown')[:40]}")
                print(f"   é”™è¯¯: {error_msg}")
                failed.append({
                    "paper_id": paper.get('id'),
                    "title": paper.get('title', 'Unknown'),
                    "error": error_msg
                })
                continue
        
        # ä¿å­˜å¤±è´¥æ—¥å¿—
        if failed:
            self._save_failed_log(failed, output_file)
            print(f"\nâš  å¤±è´¥æ—¥å¿—å·²ä¿å­˜: {output_file.parent / 'failed_papers.json'}")
        
        # æ€»ç»“
        total_processed = len(processed_ids) + processed
        print("\n" + "=" * 80)
        print("ğŸ“Š å¤„ç†æ€»ç»“")
        print("=" * 80)
        print(f"âœ“ æœ¬æ¬¡å¤„ç†: {processed}/{len(papers_to_process)} ç¯‡")
        print(f"âœ“ ç´¯è®¡å·²å¤„ç†: {total_processed}/{len(papers)} ç¯‡è®ºæ–‡")
        print(f"âœ— æœ¬æ¬¡å¤±è´¥: {len(failed)} ç¯‡")
        
        if failed:
            print(f"\nå¤±è´¥çš„è®ºæ–‡:")
            for fail_info in failed[:5]:
                print(f"  - {fail_info.get('title', 'Unknown')[:50]}")
                print(f"    ID: {fail_info.get('paper_id', 'N/A')}")
                print(f"    é”™è¯¯: {fail_info.get('error', 'Unknown')[:80]}")
            if len(failed) > 5:
                print(f"  ... è¿˜æœ‰ {len(failed)-5} ç¯‡ï¼ˆè¯¦è§ failed_papers.jsonï¼‰")
        
        # æ˜¾ç¤ºç¤ºä¾‹ï¼ˆä»æ–‡ä»¶è¯»å–æœ€åä¸€ä¸ªï¼‰
        if processed > 0 and output_file.exists():
            print("\n" + "=" * 80)
            print("ğŸ“ æ¨ç†é“¾ç¤ºä¾‹ï¼ˆæœ€åå¤„ç†çš„ä¸€ç¯‡ï¼‰")
            print("=" * 80)
            
            # è¯»å–æœ€åä¸€è¡Œ
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if lines:
                        example = json.loads(lines[-1].strip())
                        print(f"\nè®ºæ–‡: {example.get('title', 'N/A')[:60]}...")
                        print(f"DOI: {example.get('doi', 'N/A')}")
                        print(f"æœŸåˆŠ: {example.get('journal', 'N/A')}")
                        print(f"å¹´ä»½: {example.get('year', 'N/A')}")
                        
                        # æ‰å¹³åŒ–çš„æ¨ç†é“¾å­—æ®µ
                        print(f"\nğŸ¯ é—®é¢˜æ‹†è§£ (å‰200å­—ç¬¦):")
                        problem_text = example.get('problem_decomposition', 'N/A')
                        print(f"  {problem_text[:200]}...")
                        
                        print(f"\nğŸ“Š æ•°æ®éœ€æ±‚ (å‰200å­—ç¬¦):")
                        data_text = example.get('data', 'N/A')
                        print(f"  {data_text[:200]}...")
                        
                        print(f"\nğŸ”¬ ç ”ç©¶æ–¹æ³• (å‰200å­—ç¬¦):")
                        method_text = example.get('method', 'N/A')
                        print(f"  {method_text[:200]}...")
                        
                        print(f"\nâœ¨ ç»“è®º (å‰200å­—ç¬¦):")
                        conclusion_text = example.get('conclusion', 'N/A')
                        print(f"  {conclusion_text[:200]}...")
            except Exception as e:
                print(f"âš  æ— æ³•æ˜¾ç¤ºç¤ºä¾‹: {e}")
        
        print("\n" + "=" * 80)
        print("âœ… å®Œæˆï¼")
        print("=" * 80)
        
        if resume and papers_to_process:
            print("\nğŸ’¡ æç¤º:")
            print("  - å¦‚æœä¸­é€”ä¸­æ–­ï¼Œå†æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­")
            print("  - å·²å¤„ç†çš„è®ºæ–‡ä¼šè¢«è‡ªåŠ¨è·³è¿‡")
            print(f"  - è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        print("=" * 80)
        
        # ä¿å­˜æ—¥å¿—
        log_file = output_file.parent / "reasoning_extraction_log.json"
        log_data = {
            "total_papers": len(papers),
            "previously_processed": len(processed_ids),
            "this_batch_processed": processed,
            "cumulative_processed": total_processed,
            "this_batch_failed": len(failed),
            "resume_enabled": resume,
            "output_file": str(output_file),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import config
    
    parser = argparse.ArgumentParser(description="ç›´æ¥æå–ç§‘ç ”æ¨ç†é“¾")
    parser.add_argument(
        "--data-dir",
        type=str,
        default="./data/annual_review",
        help="æ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="./data/annual_review/reasoning_chains.jsonl",
        help="è¾“å‡ºæ–‡ä»¶"
    )
    parser.add_argument(
        "--max-papers",
        type=int,
        default=5,
        help="æœ€å¤šå¤„ç†å¤šå°‘ç¯‡è®ºæ–‡ï¼ˆ0=å…¨éƒ¨ï¼‰"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=config.OPENAI_API_KEY,
        help="APIå¯†é’¥"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=config.EXTRACTION_MODEL,
        help="æ¨¡å‹åç§°"
    )
    parser.add_argument(
        "--is-review",
        action="store_true",
        help="ä½¿ç”¨ç»¼è¿°æå–æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨ç ”ç©¶è®ºæ–‡æ¨¡å¼ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæå–å™¨
    extractor = DirectReasoningExtractor(
        api_key=args.api_key,
        model=args.model
    )
    
    # æ‰¹é‡å¤„ç†
    max_papers = None if args.max_papers == 0 else args.max_papers
    
    extractor.batch_process(
        data_dir=args.data_dir,
        output_file=args.output,
        max_papers=max_papers,
        is_review=args.is_review
    )

if __name__ == "__main__":
    main()
