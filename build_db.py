"""
æ··åˆç­–ç•¥å‘é‡æ•°æ®åº“æ„å»ºå™¨
æ ¸å¿ƒæ€è·¯ï¼šç”¨ problem_decomposition å»ºç«‹ç´¢å¼•ï¼Œä½†å…ƒæ•°æ®å­˜å‚¨å®Œæ•´æ¨ç†é“¾

é€‚ç”¨åœºæ™¯ï¼š
- ç”¨æˆ·è¾“å…¥å®è§‚é—®é¢˜
- éœ€è¦ç²¾å‡†åŒ¹é…é—®é¢˜å±‚é¢
- ä½†å¸Œæœ›è·å–å®Œæ•´æ¨ç†é“¾ä½œä¸ºç”Ÿæˆå‚è€ƒ
"""

import json
import chromadb
from pathlib import Path
from typing import List, Dict
from tqdm import tqdm
import config
from embedding_utils import QwenEmbedder


class HybridReasoningDBBuilder:
    """æ··åˆç­–ç•¥æ•°æ®åº“æ„å»ºå™¨"""
    
    def __init__(
        self,
        api_key: str,
        chroma_path: str = "./chroma_db",
        collection_name: str = "neuroscience"
    ):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            api_key: Qwen API key
            chroma_path: ChromaDB æŒä¹…åŒ–è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        self.embedder = QwenEmbedder(api_key=api_key, dimensions=768)  # ä½¿ç”¨768ç»´å‘é‡
        self.chroma_path = Path(chroma_path)
        self.collection_name = collection_name
        
        # åˆå§‹åŒ– ChromaDB
        print(f"\nåˆå§‹åŒ– ChromaDB: {self.chroma_path}")
        self.client = chromadb.PersistentClient(path=str(self.chroma_path))
        
        # è·å– embedding ç»´åº¦
        print("è·å– embedding ç»´åº¦...")
        self.embedding_dim = self.embedder.get_embedding_dimension()
        print(f"âœ“ Embedding ç»´åº¦: {self.embedding_dim}")
    
    def load_reasoning_chains(self, jsonl_file: Path) -> List[Dict]:
        """åŠ è½½æ¨ç†é“¾æ•°æ®"""
        print(f"\nåŠ è½½æ¨ç†é“¾æ•°æ®: {jsonl_file}")
        chains = []
        
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–æ•°æ®"):
                if line.strip():
                    chains.append(json.loads(line))
        
        print(f"âœ“ åŠ è½½äº† {len(chains)} æ¡æ¨ç†é“¾")
        return chains
    
    def load_abstracts(self, contents_file: Path) -> List[Dict]:
        """åŠ è½½æ‘˜è¦æ•°æ®ï¼ˆä»contents.jsonlï¼‰"""
        print(f"\nåŠ è½½æ‘˜è¦æ•°æ®: {contents_file}")
        abstracts = []
        
        with open(contents_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–æ•°æ®"):
                if line.strip():
                    abstracts.append(json.loads(line))
        
        print(f"âœ“ åŠ è½½äº† {len(abstracts)} æ¡æ‘˜è¦")
        return abstracts
    
    def load_metadata_dict(self, metadata_file: Path) -> Dict:
        """åŠ è½½å…ƒæ•°æ®å­—å…¸ï¼ˆä»metadata.jsonlï¼Œkeyä¸ºpaper_idï¼‰"""
        print(f"\nåŠ è½½å…ƒæ•°æ®: {metadata_file}")
        metadata_dict = {}
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="è¯»å–å…ƒæ•°æ®"):
                if line.strip():
                    meta = json.loads(line)
                    paper_id = meta.get('id')
                    if paper_id:
                        metadata_dict[paper_id] = meta
        
        print(f"âœ“ åŠ è½½äº† {len(metadata_dict)} æ¡å…ƒæ•°æ®")
        return metadata_dict
    
    def create_collection(self, reset: bool = False):
        """åˆ›å»ºæˆ–è·å– collection"""
        if reset:
            try:
                self.client.delete_collection(name=self.collection_name)
                print(f"âœ“ å·²åˆ é™¤æ—§çš„ collection: {self.collection_name}")
            except:
                pass
        
        self.collection = self.client.get_or_create_collection(
            name=self.collection_name,
            metadata={
                "description": "æ··åˆç­–ç•¥ï¼šç”¨é—®é¢˜æ£€ç´¢ï¼Œè¿”å›å®Œæ•´æ¨ç†é“¾",
                "embedding_model": "text-embedding-v4",
                "embedding_dimension": self.embedding_dim,
                "strategy": "hybrid",
                "index_field": "problem_decomposition",
                "return_field": "full_chain"
            }
        )
        
        print(f"âœ“ Collection å·²å°±ç»ª: {self.collection_name}")
        print(f"  å½“å‰æ–‡æ¡£æ•°: {self.collection.count()}")
    
    def build_database(self, reasoning_chains: List[Dict]):
        """
        æ„å»ºæ··åˆç­–ç•¥æ•°æ®åº“
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. åªå¯¹ problem_decomposition è¿›è¡Œ embeddingï¼ˆæ£€ç´¢ç´¢å¼•ï¼‰
        2. ä½†åœ¨å…ƒæ•°æ®ä¸­å­˜å‚¨å®Œæ•´çš„æ¨ç†é“¾ï¼ˆè¿”å›å†…å®¹ï¼‰
        3. æ£€ç´¢æ—¶ç²¾å‡†åŒ¹é…é—®é¢˜ï¼Œè¿”å›æ—¶è·å¾—å®Œæ•´æ¨ç†é“¾
        """
        print(f"\nå¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“...")
        print(f"ç­–ç•¥: æ··åˆç­–ç•¥ï¼ˆé—®é¢˜ç´¢å¼• + å®Œæ•´æ¨ç†é“¾å­˜å‚¨ï¼‰")
        print(f"æ€»è®ºæ–‡æ•°: {len(reasoning_chains)}")
        
        all_ids = []
        all_embeddings = []
        all_documents = []
        all_metadatas = []
        
        failed_count = 0
        
        for chain in tqdm(reasoning_chains, desc="å¤„ç†è®ºæ–‡"):
            paper_id = chain.get('paper_id', 'unknown')
            title = chain.get('title', 'Unknown Title')
            
            # æå–å››å…ƒç»„
            problem = chain.get('problem_decomposition', '')
            data = chain.get('data', '')
            method = chain.get('method', '')
            conclusion = chain.get('conclusion', '')
            
            # æ£€æŸ¥å®Œæ•´æ€§
            if not all([problem, data, method, conclusion]):
                tqdm.write(f"âš  è·³è¿‡ä¸å®Œæ•´çš„æ¨ç†é“¾: {title}")
                failed_count += 1
                continue
            
            # ğŸ”‘ å…³é”®ï¼šåªå¯¹ problem_decomposition è¿›è¡Œ embedding
            # æ„é€ æ£€ç´¢æ–‡æ¡£ï¼ˆåªåŒ…å«é—®é¢˜éƒ¨åˆ†ï¼‰
            index_text = f"""Research Title: {title}

Problem Decomposition:
{problem}"""
            
            # ç”Ÿæˆ embeddingï¼ˆåªåŸºäºé—®é¢˜ï¼‰
            embedding = self.embedder.embed_single(index_text)
            
            if embedding is None:
                tqdm.write(f"âœ— Embedding å¤±è´¥: {title}")
                failed_count += 1
                continue
            
            # ğŸ”‘ å…³é”®ï¼šåœ¨å…ƒæ•°æ®ä¸­å­˜å‚¨å®Œæ•´æ¨ç†é“¾
            reasoning_chain_dict = {
                'problem_decomposition': problem,
                'data': data,
                'method': method,
                'conclusion': conclusion
            }
            
            # å…ƒæ•°æ®ï¼ˆåŒ…å«å®Œæ•´æ¨ç†é“¾ï¼‰
            # æ³¨æ„ï¼šChromaDBä¸æ¥å—Noneå€¼ï¼Œéœ€è¦ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰é»˜è®¤å€¼
            doi_value = chain.get('doi') or ''
            year_value = chain.get('year') or 0
            citation_count_value = chain.get('citation_count') or 0
            journal_value = chain.get('journal') or ''
            is_open_access_value = chain.get('is_open_access', False)
            
            metadata = {
                'paper_id': paper_id,
                'title': title,
                'doi': doi_value,
                'year': int(year_value),  # ç¡®ä¿æ˜¯æ•´æ•°
                'citation_count': int(citation_count_value),  # ç¡®ä¿æ˜¯æ•´æ•°
                'journal': journal_value,
                'authors': json.dumps(chain.get('authors', []), ensure_ascii=False),  
                'is_open_access': bool(is_open_access_value),  # ç¡®ä¿æ˜¯å¸ƒå°”å€¼
                # å®Œæ•´çš„æ¨ç†é“¾ï¼ˆJSON å­—ç¬¦ä¸²ï¼‰
                'reasoning_chain': json.dumps(reasoning_chain_dict, ensure_ascii=False)
            }
            
            all_ids.append(paper_id)
            all_embeddings.append(embedding)
            all_documents.append(index_text)  # å­˜å‚¨ç´¢å¼•æ–‡æœ¬ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            all_metadatas.append(metadata)
            
            # æ‰¹é‡æ’å…¥
            if len(all_ids) >= 50:
                self.collection.add(
                    ids=all_ids,
                    embeddings=all_embeddings,
                    documents=all_documents,
                    metadatas=all_metadatas
                )
                all_ids, all_embeddings, all_documents, all_metadatas = [], [], [], []
        
        # æ’å…¥å‰©ä½™çš„
        if all_ids:
            self.collection.add(
                ids=all_ids,
                embeddings=all_embeddings,
                documents=all_documents,
                metadatas=all_metadatas
            )
        
        print(f"\nâœ“ æ„å»ºå®Œæˆï¼")
        print(f"  æˆåŠŸ: {self.collection.count()} ç¯‡è®ºæ–‡")
        print(f"  å¤±è´¥: {failed_count} ç¯‡è®ºæ–‡")
        print(f"\næ•°æ®ç»“æ„ï¼š")
        print(f"  - ç´¢å¼•å‘é‡: åŸºäº problem_decomposition")
        print(f"  - å…ƒæ•°æ®: å®Œæ•´æ¨ç†é“¾ï¼ˆ4ä¸ªå­—æ®µï¼‰")
    
    def build_abstract_database(self, abstracts_data: List[Dict], metadata_dict: Dict, year: int = None):
        """
        æ„å»ºæ‘˜è¦å‘é‡æ•°æ®åº“ï¼ˆç”¨äºSFNç­‰åªæœ‰æ‘˜è¦çš„ä¼šè®®è®ºæ–‡ï¼‰
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. å¯¹æ ‡é¢˜+æ‘˜è¦è¿›è¡Œembeddingï¼ˆæ£€ç´¢ç´¢å¼•ï¼‰
        2. åœ¨reasoning_chainå­—æ®µä¸­å­˜å‚¨æ‘˜è¦ä¿¡æ¯ï¼ˆä¿æŒmetadataæ ¼å¼ä¸€è‡´ï¼‰
        
        Args:
            abstracts_data: åŒ…å«æ‘˜è¦çš„æ•°æ®åˆ—è¡¨ï¼ˆä»contents.jsonlè§£æï¼‰
            metadata_dict: å…ƒæ•°æ®å­—å…¸ï¼ˆä»metadata.jsonlè§£æï¼Œkeyä¸ºpaper_idï¼‰
            year: å¹´ä»½ï¼ˆä»è·¯å¾„ä¸­æå–ï¼Œå¦‚2022/2023/2024ï¼‰
        """
        print(f"\nå¼€å§‹æ„å»ºæ‘˜è¦å‘é‡æ•°æ®åº“...")
        print(f"ç­–ç•¥: æ ‡é¢˜+æ‘˜è¦ç´¢å¼• + æ‘˜è¦ä¿¡æ¯å­˜å‚¨")
        print(f"æ€»æ‘˜è¦æ•°: {len(abstracts_data)}")
        
        all_ids = []
        all_embeddings = []
        all_documents = []
        all_metadatas = []
        
        failed_count = 0
        
        for abstract_data in tqdm(abstracts_data, desc="å¤„ç†æ‘˜è¦"):
            paper_id = abstract_data.get('id', 'unknown')
            title = abstract_data.get('title', 'Unknown Title')
            
            # æå–æ‘˜è¦å†…å®¹
            abstract_text = ""
            sections = abstract_data.get('sections', [])
            for section in sections:
                if section.get('section_title', '').lower() == 'abstract':
                    section_texts = section.get('section_text', [])
                    if section_texts:
                        # åˆå¹¶æ‰€æœ‰æ®µè½
                        abstract_text = ' '.join([str(text) for text in section_texts if text])
                    break
            
            if not abstract_text:
                tqdm.write(f"âš  è·³è¿‡æ— æ‘˜è¦çš„è®ºæ–‡: {title}")
                failed_count += 1
                continue
            
            # è·å–å…ƒæ•°æ®
            meta = metadata_dict.get(paper_id, {})
            
            # ğŸ”‘ å…³é”®ï¼šå¯¹æ ‡é¢˜+æ‘˜è¦è¿›è¡Œembedding
            index_text = f"""Title: {title}

Abstract:
{abstract_text}"""
            
            # ç”Ÿæˆ embedding
            embedding = self.embedder.embed_single(index_text)
            
            if embedding is None:
                tqdm.write(f"âœ— Embedding å¤±è´¥: {title}")
                failed_count += 1
                continue
            
            # ğŸ”‘ å…³é”®ï¼šåœ¨reasoning_chainå­—æ®µä¸­å­˜å‚¨æ‘˜è¦ä¿¡æ¯ï¼ˆä¿æŒmetadataæ ¼å¼ä¸€è‡´ï¼‰
            reasoning_chain_dict = {
                'abstract': abstract_text
            }
            
            # å…ƒæ•°æ®ï¼ˆæ ¼å¼ä¸æ¨ç†é“¾æ•°æ®ä¿æŒä¸€è‡´ï¼‰
            # æ³¨æ„ï¼šChromaDBä¸æ¥å—Noneå€¼ï¼Œéœ€è¦ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰é»˜è®¤å€¼
            doi_value = meta.get('doi') or ''
            year_value = year or meta.get('publish_year') or 0
            citation_count_value = meta.get('citation_count') or 0
            journal_value = meta.get('journal') or 'SFN'
            is_open_access_value = meta.get('is_open_access', False)
            
            metadata = {
                'paper_id': paper_id,
                'title': title,
                'doi': doi_value,
                'year': int(year_value),  # ç¡®ä¿æ˜¯æ•´æ•°
                'citation_count': int(citation_count_value),  # ç¡®ä¿æ˜¯æ•´æ•°
                'journal': journal_value,
                'authors': json.dumps(meta.get('authors', []), ensure_ascii=False),
                'is_open_access': bool(is_open_access_value),  # ç¡®ä¿æ˜¯å¸ƒå°”å€¼
                # åœ¨reasoning_chainå­—æ®µä¸­å­˜å‚¨æ‘˜è¦ä¿¡æ¯
                'reasoning_chain': json.dumps(reasoning_chain_dict, ensure_ascii=False)
            }
            
            all_ids.append(paper_id)
            all_embeddings.append(embedding)
            all_documents.append(index_text)  # å­˜å‚¨ç´¢å¼•æ–‡æœ¬ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            all_metadatas.append(metadata)
            
            # æ‰¹é‡æ’å…¥
            if len(all_ids) >= 50:
                self.collection.add(
                    ids=all_ids,
                    embeddings=all_embeddings,
                    documents=all_documents,
                    metadatas=all_metadatas
                )
                all_ids, all_embeddings, all_documents, all_metadatas = [], [], [], []
        
        # æ’å…¥å‰©ä½™çš„
        if all_ids:
            self.collection.add(
                ids=all_ids,
                embeddings=all_embeddings,
                documents=all_documents,
                metadatas=all_metadatas
            )
        
        print(f"\nâœ“ æ„å»ºå®Œæˆï¼")
        print(f"  æˆåŠŸ: {self.collection.count()} ç¯‡æ‘˜è¦")
        print(f"  å¤±è´¥: {failed_count} ç¯‡æ‘˜è¦")
        print(f"\næ•°æ®ç»“æ„ï¼š")
        print(f"  - ç´¢å¼•å‘é‡: åŸºäº æ ‡é¢˜+æ‘˜è¦")
        print(f"  - å…ƒæ•°æ®: reasoning_chainå­—æ®µå­˜å‚¨æ‘˜è¦ä¿¡æ¯")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ„å»ºæ··åˆç­–ç•¥å‘é‡æ•°æ®åº“ï¼ˆé—®é¢˜ç´¢å¼• + å®Œæ•´æ¨ç†é“¾ï¼‰"
    )
    parser.add_argument(
        "--input",
        type=str,
        default="data/neuron/reasoning_chains.jsonl",
        help="æ¨ç†é“¾ JSONL æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--chroma-path",
        type=str,
        default="./chroma_db",
        help="ChromaDB å­˜å‚¨è·¯å¾„"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="é‡ç½®å·²å­˜åœ¨çš„æ•°æ®åº“"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=None,
        help="Qwen API Key"
    )
    parser.add_argument(
        "--abstract-mode",
        action="store_true",
        help="æ‘˜è¦æ¨¡å¼ï¼šå¤„ç†åªæœ‰æ‘˜è¦çš„æ•°æ®ï¼ˆå¦‚SFNä¼šè®®è®ºæ–‡ï¼‰"
    )
    parser.add_argument(
        "--contents",
        type=str,
        default=None,
        help="æ‘˜è¦æ¨¡å¼ï¼šcontents.jsonlæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--metadata",
        type=str,
        default=None,
        help="æ‘˜è¦æ¨¡å¼ï¼šmetadata.jsonlæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--year",
        type=int,
        default=None,
        help="æ‘˜è¦æ¨¡å¼ï¼šå¹´ä»½ï¼ˆå¦‚2022/2023/2024ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è·å– API key
    api_key = args.api_key or config.OPENAI_API_KEY
    
    # åˆ›å»ºæ„å»ºå™¨
    builder = HybridReasoningDBBuilder(
        api_key=api_key,
        chroma_path=args.chroma_path
    )
    
    # åˆ›å»º collection
    builder.create_collection(reset=args.reset)
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
    if args.abstract_mode:
        # æ‘˜è¦æ¨¡å¼
        print("="*80)
        print("æ‘˜è¦æ¨¡å¼ï¼šå¤„ç†åªæœ‰æ‘˜è¦çš„æ•°æ®")
        print("ç­–ç•¥è¯´æ˜ï¼š")
        print("  1. ç´¢å¼•ï¼šå¯¹ æ ‡é¢˜+æ‘˜è¦ å»ºç«‹å‘é‡ç´¢å¼•")
        print("  2. å­˜å‚¨ï¼šreasoning_chainå­—æ®µä¸­å­˜å‚¨æ‘˜è¦ä¿¡æ¯")
        print("  3. æ£€ç´¢ï¼šç”¨æˆ·é—®é¢˜åŒ¹é…æ ‡é¢˜å’Œæ‘˜è¦å†…å®¹")
        print("="*80)
        
        if not args.contents or not args.metadata:
            print("âŒ é”™è¯¯ï¼šæ‘˜è¦æ¨¡å¼éœ€è¦æŒ‡å®š --contents å’Œ --metadata")
            return
        
        # åŠ è½½æ•°æ®
        abstracts = builder.load_abstracts(Path(args.contents))
        metadata_dict = builder.load_metadata_dict(Path(args.metadata))
        
        # ä»è·¯å¾„ä¸­æå–å¹´ä»½ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        year = args.year
        if not year:
            # å°è¯•ä»è·¯å¾„ä¸­æå–å¹´ä»½
            import re
            path_str = str(args.contents)
            year_match = re.search(r'/(\d{4})/', path_str)
            if year_match:
                year = int(year_match.group(1))
        
        # æ„å»ºæ•°æ®åº“
        builder.build_abstract_database(abstracts, metadata_dict, year=year)
    else:
        # æ¨ç†é“¾æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        print("="*80)
        print("æ¨ç†é“¾æ¨¡å¼ï¼šå¤„ç†å®Œæ•´æ¨ç†é“¾æ•°æ®")
        print("ç­–ç•¥è¯´æ˜ï¼š")
        print("  1. ç´¢å¼•ï¼šåªå¯¹ problem_decomposition å»ºç«‹å‘é‡ç´¢å¼•")
        print("  2. å­˜å‚¨ï¼šå…ƒæ•°æ®ä¸­ä¿å­˜å®Œæ•´çš„å››å…ƒç»„")
        print("  3. æ£€ç´¢ï¼šç”¨æˆ·é—®é¢˜ç²¾å‡†åŒ¹é… problemï¼Œé¿å…è¢«æ–¹æ³•ç¨€é‡Š")
        print("  4. è¿”å›ï¼šè·å–å®Œæ•´æ¨ç†é“¾ä½œä¸ºç”Ÿæˆå‚è€ƒ")
        print("="*80)
        
        # åŠ è½½æ•°æ®
        chains = builder.load_reasoning_chains(Path(args.input))
        
        # æ„å»ºæ•°æ®åº“
        builder.build_database(chains)
    
    print("\n" + "="*80)
    print("âœ“ å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    print("="*80)
    print(f"\næ•°æ®åº“ä½ç½®: {args.chroma_path}")
    print(f"Collection åç§°: neuroscience")
    
if __name__ == "__main__":
    main()
